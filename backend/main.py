import os
import tempfile
import whisper
import json
from dotenv import load_dotenv
from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from openai import OpenAI
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_pinecone import PineconeVectorStore
from langchain_core.messages import HumanMessage

# ==== í™˜ê²½ ì„¤ì • ====
load_dotenv()

UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV")

if not UPSTAGE_API_KEY:
    raise ValueError("UPSTAGE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Whisper ëª¨ë¸ ë¡œë“œ
whisper_model = whisper.load_model("base")

# Solar LLM ì´ˆê¸°í™” (ChatUpstage)
solar_model = ChatUpstage(api_key=UPSTAGE_API_KEY, model="solar-pro")

# Pinecone ë²¡í„° DB ì´ˆê¸°í™”
embedding_model = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="embedding-query")
index_name = "langchain-demo"
vectorstore = PineconeVectorStore(
    index_name=index_name,
    embedding=embedding_model,
    pinecone_api_key=PINECONE_API_KEY,
)

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ë°”ë”” ì •ì˜
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: list[Message]

@app.get("/")
async def root():
    return {"message": "EmergencyAI Backend is running!"}

@app.post("/transcribe")
async def transcribe(audio: UploadFile = File(...)):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp:
            tmp.write(await audio.read())
            tmp_path = tmp.name
        result = whisper_model.transcribe(tmp_path, language="ko")
        os.remove(tmp_path)
        return {"text": result["text"]}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/chat")
async def chat_with_ai(request: ChatRequest):
    if not request.messages:
        return {"error": "Messages list is empty"}

    try:
        # ì „ì²´ ëŒ€í™” ê¸°ë¡
         conversation = [{"role": msg.role, "content": msg.content} for msg in request.messages]

        # âœ… ê°€ì¥ ìµœê·¼ user ì…ë ¥ ì¶”ì¶œ
         patient_state = [msg.content for msg in request.messages if msg.role == "user"][-1]
         query = generate_search_query(patient_state)

        # Step 2: ë²¡í„° ê²€ìƒ‰ + í•„í„°
         docs_with_scores = vectorstore.similarity_search_with_score(query, k=5)
         docs = [doc for doc, score in docs_with_scores]
         scores = [score for doc, score in docs_with_scores]

        # Step 3: ì¹˜ë£Œ ìš”ì•½ ë° ì¶”ê°€ ì •ë³´ ìœ ë¬´ íŒë‹¨
         rag_response, required_info = llm_infer_treatment_and_missing_info(
            docs, scores, patient_state, threshold=0.2, solar_model=solar_model
        )

         conversation.append({"role": "assistant", "content": rag_response})

         
        # Step 5: í•„ìš”ì‹œ ìµœì¢… ë¸Œë¦¬í•‘ ìƒì„±
         should_end = not required_info
         medical_brief = None
         if should_end:
            medical_brief = generate_medical_brief(conversation, solar_model)

         return {
            "response": rag_response,
            "should_end": should_end,
            "conversation": conversation,
            "medical_brief": medical_brief
         }

    except Exception as e:
        print(f"Solar LLM error: {e}")
        return {"error": f"An error occurred: {e}", "should_end": False}

# ===================== í™•ì¥: RAG ê¸°ë°˜ ì¹˜ë£Œ ì •ë³´ ì¶”ë¡  =====================

def generate_search_query(patient_state):
    return patient_state

def llm_infer_treatment_and_missing_info(docs, scores, current_state, threshold, solar_model):
    try:
        filtered_docs = [docs[i] for i in range(len(scores)) if scores[i] >= threshold]
        if not filtered_docs:
            return "ê´€ë ¨ëœ ë…¼ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", False

        reference_list = [doc.metadata["source"] for doc in filtered_docs if "source" in doc.metadata]
        treatment_by_pdf = {}

        for title in reference_list:
            with open(f'./references/{title}.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                pdf_content = data.get("full_text", "")
                prompt = f"""
                ë„ˆëŠ” ì‘ê¸‰ì˜í•™ ë…¼ë¬¸ì´ë‚˜ ë¬¸ì„œë¥¼ ë¶„ì„í•´ì„œ 'ì¹˜ë£Œ(ì²˜ì¹˜)' ê´€ë ¨ ë‚´ìš©ë§Œ ìš”ì•½í•˜ëŠ” AIì•¼.
                ë‹¤ìŒì€ '{title}'ë¼ëŠ” ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì´ì•¼:
                \"\"\"{pdf_content}\"\"\"
                ì´ ë¬¸ì„œì—ì„œ 'ì¹˜ë£Œ ë˜ëŠ” ì²˜ì¹˜ì— ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©'ë§Œ ë½‘ì•„ì„œ ìš”ì•½í•´ì¤˜.
                """
                response = solar_model.invoke([HumanMessage(content=prompt)])
                treatment_by_pdf[title] = response.content.strip()

        if not treatment_by_pdf:
            return "ê´€ë ¨ëœ PDF ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", False

        all_treatments = "\n\n".join([
            f"[{src}]\n{summary}" for src, summary in treatment_by_pdf.items()
        ])

        reasoning_prompt = f"""
        ë‹¤ìŒì€ ì—¬ëŸ¬ ì‘ê¸‰ì˜í•™ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì¹˜ë£Œ ê´€ë ¨ ìš”ì•½ ì •ë³´ì•¼:
        {all_treatments}

        í˜„ì¬ ì‘ê¸‰í™˜ìì˜ ìƒíƒœëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
        \"\"\"{current_state}\"\"\"

        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™˜ìì—ê²Œ ì–´ë–¤ ì¹˜ë£Œë¥¼ í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê³ ,
        ì¶”ê°€ë¡œ ì•Œì•„ì•¼ í•  ì •ë³´ê°€ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì¤˜.
        ì¶”ê°€ ì •ë³´ê°€ í•„ìš” ì—†ë‹¤ë©´ ë°˜ë“œì‹œ \"ì¶”ê°€ ì •ë³´ í•„ìš” ì—†ìŒ\"ì´ë¼ê³  ë§í•´ì¤˜.
        """
        final_response = solar_model.invoke([HumanMessage(content=reasoning_prompt)])
        output = final_response.content.strip()

        additional_info_needed = not (
            "ì¶”ê°€ ì •ë³´ í•„ìš” ì—†ìŒ" in output or "ë” í•„ìš”í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤" in output
        )

        return output, additional_info_needed
    
    except Exception as e:
        print(f"[RAG ì²˜ë¦¬ ì˜¤ë¥˜]: {e}")
        return "ì¹˜ë£Œ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", False


def generate_medical_brief(conversation, solar_model):
    dialogue = ""
    for turn in conversation:
        role = "ğŸ‘¨â€âš•ï¸ì‘ê¸‰êµ¬ì¡°ì‚¬" if turn["role"] == "user" else "ğŸ¤–AI"
        dialogue += f"{role}: {turn['content']}\n"

    prompt = f"""
    ë„ˆëŠ” ë³‘ì›ì— í™˜ì ìƒíƒœë¥¼ ì •ë¦¬í•´ì„œ ì „ë‹¬í•˜ëŠ” ì‘ê¸‰êµ¬ì¡°ì‚¬ AIì•¼.

    ì•„ë˜ëŠ” í˜„ì¥ì—ì„œ ì‘ê¸‰êµ¬ì¡°ì‚¬ì™€ AI ê°„ì˜ ì‹¤ì œ ëŒ€í™” ë¡œê·¸ì•¼:
    -------------------------------
    {dialogue}
    -------------------------------

    ìœ„ ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³‘ì›ì— ì „ë‹¬í•  ê³µì‹ ë¸Œë¦¬í•‘ ë¬¸ì„œë¥¼ ì‘ì„±í•´ì¤˜.
    í¬ë§·ì€ ë‹¤ìŒê³¼ ê°™ì•„:

    ---
    ğŸ§¾ [í™˜ì ìƒíƒœ ìš”ì•½]
    - (ì²˜ìŒ ë³´ê³ ëœ ìƒíƒœ ìš”ì•½)

    ğŸ’‰ [AIê°€ ì œì•ˆí•œ ì¹˜ë£Œ ë° ì²˜ì¹˜ ìš”ì•½]
    - (ì¹˜ë£Œ ìš”ì•½ ìš”ì )

    â— [í˜„ì¥ì—ì„œ ìˆ˜ì§‘ëœ ì¶”ê°€ ì •ë³´]
    - (êµ¬ì¡°ì‚¬ê°€ ë‚˜ì¤‘ì— ì…ë ¥í•œ ë³´ì™„ ì •ë³´)

    ğŸ“Œ [ìµœì¢… íŒë‹¨ ë˜ëŠ” ì´ì†¡ ì§€ì‹œ ìš”ì•½]
    - (AIê°€ ìµœì¢…ì ìœ¼ë¡œ ë‚´ë¦° íŒë‹¨ ë° ë³‘ì› ì´ì†¡ í•„ìš” ì—¬ë¶€)
    ---
    """
    response = solar_model.invoke([HumanMessage(content=prompt)])
    return response.content.strip()
