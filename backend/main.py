import os
import tempfile
import whisper
import json
from dotenv import load_dotenv
from fastapi import FastAPI, File, UploadFile, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from openai import OpenAI
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_pinecone import PineconeVectorStore
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain.prompts import ChatPromptTemplate
from fastapi import HTTPException

# ==== í™˜ê²½ ì„¤ì • ====
load_dotenv()

UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV")

if not UPSTAGE_API_KEY:
    raise ValueError("UPSTAGE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Whisper ëª¨ë¸ ë¡œë“œ
whisper_model = whisper.load_model("base")

# Solar LLM ì´ˆê¸°í™” (ChatUpstage)
solar_model = ChatUpstage(api_key=UPSTAGE_API_KEY, model="solar-pro")

# Pinecone ë²¡í„° DB ì´ˆê¸°í™”
embedding_model = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="embedding-query")
index_name = "emergency-medical-kb"
vectorstore = PineconeVectorStore(
    index_name=index_name,
    embedding=embedding_model,
    pinecone_api_key=PINECONE_API_KEY,
)

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ë°”ë”” ì •ì˜
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: list[Message]

class BriefingRequest(BaseModel):
    conversation: list[Message]

@app.get("/")
async def root():
    return {"message": "EmergencyAI Backend is running!"}

@app.post("/transcribe")
async def transcribe(audio: UploadFile = File(...)):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp:
            tmp.write(await audio.read())
            tmp_path = tmp.name
        result = whisper_model.transcribe(tmp_path, language="ko")
        os.remove(tmp_path)
        return {"text": result["text"]}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

import re

def format_ai_response(text: str) -> str:
    # Add blank lines before numbered items
    return re.sub(r"(?<!\n)([1-4]\.)", r"\n\n\1", text).strip()

@app.post("/chat")
async def chat_with_ai(request: ChatRequest):
    if not request.messages:
        return {"error": "Messages list is empty"}

    try:
        user_turns = sum(1 for msg in request.messages if msg.role == "user")

        # Build message history
        messages = []
        for msg in request.messages:
            if msg.role == "user":
                messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                messages.append(AIMessage(content=msg.content))

        # RAG ì²˜ë¦¬ ì¶”ê°€
        patient_state = "\n".join([msg.content for msg in messages if isinstance(msg, HumanMessage)])
        docs_with_scores = vectorstore.similarity_search_with_score(patient_state, k=1)
        docs = [doc for doc, score in docs_with_scores]
        scores = [score for doc, score in docs_with_scores]
        
        rag_response, required_info = llm_infer_treatment_and_missing_info(
            docs, scores, patient_state, threshold=0.2, solar_model=solar_model
        )

        messages.append(AIMessage(content=rag_response))

        # Generate AI response
        response = solar_model.invoke(messages)
        rag_response = format_ai_response(response.content.strip())

        # 4í„´ì´ë©´ ì¢…ë£Œí•˜ê³  ë¸Œë¦¬í•‘ ìƒì„±
        should_end = user_turns == 4
        medical_brief = None
       
        return {
            "response": rag_response,
            "should_end": should_end,
            "conversation": [m.dict() if hasattr(m, 'dict') else {"role": m.type, "content": m.content} for m in messages],
            "medical_brief": medical_brief
        }

    except Exception as e:
        print(f"Solar LLM error: {e}")
        return {"error": f"An error occurred: {e}", "should_end": False}

# ===================== í™•ì¥: RAG ê¸°ë°˜ ì¹˜ë£Œ ì •ë³´ ì¶”ë¡  =====================

def generate_search_query(patient_state):
    return patient_state

def llm_infer_treatment_and_missing_info(docs, scores, current_state, threshold, solar_model):
    try:
        filtered_docs = [docs[i] for i in range(len(scores)) if scores[i] >= threshold]
        if not filtered_docs:
            return "ê´€ë ¨ëœ ë…¼ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", False

        reference_list = []
        for doc in filtered_docs:
            if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict):
                source = doc.metadata.get('source', '')
                if source:
                    reference_list.append(source.split('.')[0])
            else:
                print(f"[ê²½ê³ ] ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì—†ìŒ: {doc}")

        treatment_by_pdf = {}

        for i, title in enumerate(reference_list):
            print('------------------------------')
            print(title)
            try:
                with open(f'../reference/{title}.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    pdf_content = data.get("full_text", "")
            except FileNotFoundError:
                print(f"[ê²½ê³ ] íŒŒì¼ ì—†ìŒ: {title}.json â€” Solar LLMì´ ì‚¬ìš©ì ìƒíƒœë§Œ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨")
                continue  # PDFê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œëŠ” ê±´ë„ˆëœ€

            prompt = f"""
            ë„ˆëŠ” ì‘ê¸‰ì˜í•™ ë…¼ë¬¸ì´ë‚˜ ë¬¸ì„œë¥¼ ë¶„ì„í•´ì„œ 'ì¹˜ë£Œ(ì²˜ì¹˜)' ê´€ë ¨ ë‚´ìš©ë§Œ ìš”ì•½í•˜ëŠ” AIì•¼.
            ë‹¤ìŒì€ '{title}'ë¼ëŠ” ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì´ì•¼:
            \"\"\"{pdf_content}\"\"\" 
            ì´ ë¬¸ì„œì—ì„œ 'ì¹˜ë£Œ ë˜ëŠ” ì²˜ì¹˜ì— ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©'ë§Œ ë½‘ì•„ì„œ ìš”ì•½í•´ì¤˜.
            """
            response = solar_model.invoke([HumanMessage(content=prompt)])
            print(response.content.strip())
            treatment_by_pdf[title] = response.content.strip()

        # PDF ê¸°ë°˜ ìš”ì•½ì´ í•˜ë‚˜ë„ ì—†ì„ ê²½ìš°, ì§ì ‘ ìƒíƒœ ê¸°ë°˜ ì‘ë‹µ
        if not treatment_by_pdf:
            fallback_prompt = f"""
            ë‹¤ìŒì€ í˜„ì¬ ì‘ê¸‰í™˜ìì˜ ìƒíƒœì•¼:
            \"\"\"{current_state}\"\"\" 

            ìœ„ ìƒíƒœë¥¼ ë³´ê³  ì–´ë–¤ ì²˜ì¹˜ë‚˜ ì¹˜ë£Œë¥¼ í•´ì•¼ í• ì§€ ì„¤ëª…í•´ì¤˜.
            ì¶”ê°€ë¡œ ì•Œì•„ì•¼ í•  ì •ë³´ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒë„ êµ¬ì²´ì ìœ¼ë¡œ ë§í•´ì¤˜.
            """
            fallback_response = solar_model.invoke([HumanMessage(content=fallback_prompt)])
            output = fallback_response.content.strip()
            additional_info_needed = not (
                "ì¶”ê°€ ì •ë³´ í•„ìš” ì—†ìŒ" in output or "ë” í•„ìš”í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤" in output
            )
            return output, additional_info_needed

        # ì •ìƒì ìœ¼ë¡œ ì¹˜ë£Œ ìš”ì•½ëœ ê²½ìš°
        all_treatments = "\n\n".join([
            f"[{src}]\n{summary}" for src, summary in treatment_by_pdf.items()
        ])
        reasoning_prompt = f"""
        ë‹¤ìŒì€ ì—¬ëŸ¬ ì‘ê¸‰ì˜í•™ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì¹˜ë£Œ ê´€ë ¨ ìš”ì•½ ì •ë³´ì•¼:
        {all_treatments}

        í˜„ì¬ ì‘ê¸‰í™˜ìì˜ ìƒíƒœëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
        \"\"\"{current_state}\"\"\" 

        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™˜ìì—ê²Œ ì–´ë–¤ ì¹˜ë£Œë¥¼ í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê³ ,
        ì¶”ê°€ë¡œ ì•Œì•„ì•¼ í•  ì •ë³´ê°€ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì¤˜.
        ì¶”ê°€ ì •ë³´ê°€ í•„ìš” ì—†ë‹¤ë©´ ë°˜ë“œì‹œ "ì¶”ê°€ ì •ë³´ í•„ìš” ì—†ìŒ"ì´ë¼ê³  ë§í•´ì¤˜.
        """
        final_response = solar_model.invoke([HumanMessage(content=reasoning_prompt)])
        output = final_response.content.strip()

        additional_info_needed = not (
            "ì¶”ê°€ ì •ë³´ í•„ìš” ì—†ìŒ" in output or "ë” í•„ìš”í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤" in output
        )
        print(output)
        return output, additional_info_needed

    except Exception as e:
        print(f"[RAG ì²˜ë¦¬ ì˜¤ë¥˜]: {e}")
        return "ì¹˜ë£Œ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", False

def generate_medical_brief(conversation, solar_model):
    dialogue = ""
    for turn in conversation:
        role = "ğŸ‘¨â€âš•ï¸ì‘ê¸‰êµ¬ì¡°ì‚¬" if turn["role"] == "user" else "ğŸ¤–AI"
        dialogue += f"{role}: {turn['content']}\n"

    prompt = f"""
    ë„ˆëŠ” ë³‘ì›ì— í™˜ì ìƒíƒœë¥¼ ì •ë¦¬í•´ì„œ ì „ë‹¬í•˜ëŠ” ì‘ê¸‰êµ¬ì¡°ì‚¬ AIì•¼.

    ì•„ë˜ëŠ” í˜„ì¥ì—ì„œ ì‘ê¸‰êµ¬ì¡°ì‚¬ì™€ AI ê°„ì˜ ì‹¤ì œ ëŒ€í™” ë¡œê·¸ì•¼:
    -------------------------------
    {dialogue}
    -------------------------------

    ìœ„ ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³‘ì›ì— ì „ë‹¬í•  ê³µì‹ ë¸Œë¦¬í•‘ ë¬¸ì„œë¥¼ ì‘ì„±í•´ì¤˜.
    í¬ë§·ì€ ë‹¤ìŒê³¼ ê°™ì•„:

    ---
    ğŸ§¾ [í™˜ì ìƒíƒœ ìš”ì•½]
    - (ì²˜ìŒ ë³´ê³ ëœ ìƒíƒœ ìš”ì•½)

    ğŸ’‰ [AIê°€ ì œì•ˆí•œ ì¹˜ë£Œ ë° ì²˜ì¹˜ ìš”ì•½]
    - (ì¹˜ë£Œ ìš”ì•½ ìš”ì )

    â— [í˜„ì¥ì—ì„œ ìˆ˜ì§‘ëœ ì¶”ê°€ ì •ë³´]
    - (êµ¬ì¡°ì‚¬ê°€ ë‚˜ì¤‘ì— ì…ë ¥í•œ ë³´ì™„ ì •ë³´)

    ğŸ“Œ [ìµœì¢… íŒë‹¨ ë˜ëŠ” ì´ì†¡ ì§€ì‹œ ìš”ì•½]
    - (AIê°€ ìµœì¢…ì ìœ¼ë¡œ ë‚´ë¦° íŒë‹¨ ë° ë³‘ì› ì´ì†¡ í•„ìš” ì—¬ë¶€)
    ---
    """
    response = solar_model.invoke([HumanMessage(content=prompt)])
    return response.content.strip()

@app.post("/generate_medical_brief")
async def generate_medical_brief(request: Request):
    try:
        data = await request.json()
        conversation = data.get("conversation", [])
        
        if not conversation:
            raise HTTPException(status_code=400, detail="ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëŒ€í™” ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        conversation_text = "\n".join([
            f"{'ì‘ê¸‰êµ¬ì¡°ì‚¬' if msg['role'] == 'user' else 'AI'}: {msg['content']}"
            for msg in conversation
        ])
        
        print("ëŒ€í™” ë‚´ìš©:", conversation_text)  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        # LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""ë„ˆëŠ” ì‘ê¸‰ìƒí™©ì„ ë¶„ì„í•˜ê³  ì˜ë£Œ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ëŠ” AI ì˜ë£Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì˜ë£Œ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©:
{conversation_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:

1. í™˜ì ìƒíƒœ ìš”ì•½
- í™˜ìì˜ ê¸°ë³¸ ì •ë³´ (ë‚˜ì´, ì„±ë³„)
- í˜„ì¬ ìƒíƒœì™€ ì£¼ìš” ì¦ìƒ
- ì˜ì‹ ìƒíƒœ
- ìƒì²´ì§•í›„ (ì•Œë ¤ì§„ ê²½ìš°)

2. ì¹˜ë£Œ ë° ì²˜ì¹˜ ìš”ì•½
- í˜„ì¬ê¹Œì§€ ì‹œí–‰ëœ ì²˜ì¹˜
- ì¶”ê°€ë¡œ í•„ìš”í•œ ì¹˜ë£Œ
- ì•½ë¬¼ íˆ¬ì—¬ ë‚´ì—­ (ìˆëŠ” ê²½ìš°)

3. ì¶”ê°€ ì •ë³´
- ì•Œë ˆë¥´ê¸°ë‚˜ ê¸°ì €ì§ˆí™˜
- ì‚¬ê³ /ì¦ìƒ ë°œìƒ ì‹œì 
- íŠ¹ì´ì‚¬í•­ì´ë‚˜ ì£¼ì˜ì 

4. ìµœì¢… íŒë‹¨
- ì˜ì‹¬ë˜ëŠ” ì§ˆí™˜/ìƒíƒœ
- ì‘ê¸‰ë„ í‰ê°€
- ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

ê° ì„¹ì…˜ì€ ë°˜ë“œì‹œ bullet point(-)ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª©ë“¤ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
ê° í•­ëª©ì€ êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ëª¨ë“  ì„¹ì…˜ì— ëŒ€í•´ ìµœì†Œí•œ í•˜ë‚˜ ì´ìƒì˜ í•­ëª©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        # Solar LLM í˜¸ì¶œ
        response = solar_model.invoke([HumanMessage(content=prompt)])
        content = response.content.strip()
        
        print("LLM ì‘ë‹µ:", content)  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        return {"content": content}
        
    except Exception as e:
        print(f"ë¸Œë¦¬í•‘ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/send_to_hospital")
async def send_to_hospital(briefing_data: dict):
    try:
        # TODO: ì‹¤ì œ ë³‘ì› ì „ì†¡ ë¡œì§ êµ¬í˜„
        # í˜„ì¬ëŠ” ì„±ê³µ ì‘ë‹µë§Œ ë°˜í™˜
        return {"status": "success", "message": "ë³‘ì›ìœ¼ë¡œ ë¸Œë¦¬í•‘ì´ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"ë³‘ì› ì „ì†¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
        )
