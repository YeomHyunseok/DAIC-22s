import os
import tempfile
import whisper
import json
from dotenv import load_dotenv
from fastapi import FastAPI, File, UploadFile, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from openai import OpenAI
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_pinecone import PineconeVectorStore
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain.prompts import ChatPromptTemplate
from fastapi import HTTPException

# ==== í™˜ê²½ ì„¤ì • ====
load_dotenv()

UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV")

if not UPSTAGE_API_KEY:
    raise ValueError("UPSTAGE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Whisper ëª¨ë¸ ë¡œë“œ
whisper_model = whisper.load_model("base")

# Solar LLM ì´ˆê¸°í™” (ChatUpstage)
solar_model = ChatUpstage(api_key=UPSTAGE_API_KEY, model="solar-pro")

# Pinecone ë²¡í„° DB ì´ˆê¸°í™”
embedding_model = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model="embedding-query")
index_name = "langchain-demo"
vectorstore = PineconeVectorStore(
    index_name=index_name,
    embedding=embedding_model,
    pinecone_api_key=PINECONE_API_KEY,
)

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ë°”ë”” ì •ì˜
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: list[Message]

class BriefingRequest(BaseModel):
    conversation: list[Message]

@app.get("/")
async def root():
    return {"message": "EmergencyAI Backend is running!"}

@app.post("/transcribe")
async def transcribe(audio: UploadFile = File(...)):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp:
            tmp.write(await audio.read())
            tmp_path = tmp.name
        result = whisper_model.transcribe(tmp_path, language="ko")
        os.remove(tmp_path)
        return {"text": result["text"]}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

import re

def format_ai_response(text: str) -> str:
    # Add blank lines before numbered items
    return re.sub(r"(?<!\n)([1-4]\.)", r"\n\n\1", text).strip()

@app.post("/chat")
async def chat_with_ai(request: ChatRequest):
    if not request.messages:
        return {"error": "Messages list is empty"}

    try:
        user_turns = sum(1 for msg in request.messages if msg.role == "user")

        # Base role prompt for all turns
        base_prompt = """
        [ROLE]
        You are an emergency AI assistant that supports paramedics during real-time incidents.
        The human speaking to you is a paramedic reporting from the field, not a patient.

        You must strictly follow a 4-turn structure.
        Each turn has a specific role and output format, which you must obey.

        [RESPONSE FORMAT FOR ALL TURNS]
        - Always write numbered responses like this:
            1. â€¦
            2. â€¦
            3. â€¦
            4. â€¦
        - Leave exactly one blank line between each numbered line.
        - Each numbered line should be a short paragraph (1â€“3 sentences).
        - Do NOT include section titles such as "Summary" or "Additional Questions."
        - Do NOT generate responses for later turns ahead of time.
        - Always stay within the current turn only.
        """

        # Append turn-specific behavior
        if user_turns == 0:
            turn_instructions = """
            [TURN 1]

            IMPORTANT: This is the first turn of the conversation. The paramedic has just reported a general symptom like "difficulty breathing".

            Your job:
            - DO NOT give any medical advice, clinical instruction, or emergency procedure.
            - DO NOT describe any action like "place the patient comfortably" or "give oxygen".
            - DO NOT mention Turn 2, 3, or 4.
            - Only ask questions to gather essential patient information.

            You MUST ask **all** of the following:
                1. What is the patient's exact age and sex?
                2. Is the patient conscious or unconscious?
                3. Where is the pain or discomfort located, if any?
                4. Is there any visible bleeding?
                5. Are there any signs of shock (cold skin, sweating, confusion)?
                6. When did the symptom (e.g. breathing difficulty) begin?

            Format Rules:
            - Your response must contain exactly 4 numbered lines starting with 1., 2., 3., 4.
            - Each line must be a short question, in natural conversational tone.
            - DO NOT include section titles or explanations.
            - Leave one blank line between each question.
            """

        elif user_turns == 1:
            turn_instructions = """
            [TURN 2]
            The paramedic has answered your initial questions.

            Your task:
            - Briefly explain initial emergency actions based on the condition (e.g. airway check, bleeding control)
            - Ask for any critical missing information (e.g. time/location of accident, medications, underlying conditions)

            Do NOT predict diagnoses or provide final summaries.
            Do NOT include Turn 3 or Turn 4 content.
            Stay within Turn 2 only.
            """
        elif user_turns == 2:
            turn_instructions = """
            [TURN 3]
            The paramedic has provided additional patient details.

            Your task:
            - Suggest possible conditions or injuries based on reported symptoms
            - Recommend a continued course of action
            - Ask for any final details needed before ending the case

            Do NOT give transport instructions yet.
            Do NOT conclude the conversation.
            Only write content for Turn 3.
            """
        else:
            turn_instructions = """
            [TURN 4]
            You now have enough information to make a final decision.

            Your task:
            - Provide final treatment guidance
            - Clearly state whether the patient should be transported to a hospital
            - End the interaction with a closing sentence

            Do NOT repeat earlier turn content.
            Only write content for Turn 4.
            """

        system_content = base_prompt + turn_instructions

        # Build message history
        messages = [SystemMessage(content=system_content)]
        for msg in request.messages:
            if msg.role == "user":
                messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                messages.append(AIMessage(content=msg.content))

        # Generate AI response
        response = solar_model.invoke(messages)
        rag_response = format_ai_response(response.content.strip())

        # should_end = user_turns >= 4 and user_turns <=2 and any(
        #     keyword in rag_response.lower() for keyword in [
        #         "ì´ì†¡", "ë³‘ì›", "ì „ì†¡"
        #     ]
        # )
        should_end = user_turns == 4

        return {
            "response": rag_response,
            "should_end": should_end,
            "conversation": [m.dict() if hasattr(m, 'dict') else {"role": m.type, "content": m.content} for m in messages],
            "medical_brief": None
        }

    except Exception as e:
        print(f"Solar LLM error: {e}")
        return {"error": f"An error occurred: {e}", "should_end": False}



# ===================== í™•ì¥: RAG ê¸°ë°˜ ì¹˜ë£Œ ì •ë³´ ì¶”ë¡  =====================

def generate_search_query(patient_state):
    return patient_state

def llm_infer_treatment_and_missing_info(docs, scores, current_state, threshold, solar_model):
    try:
        filtered_docs = [docs[i] for i in range(len(scores)) if scores[i] >= threshold]
        if not filtered_docs:
            return "ê´€ë ¨ëœ ë…¼ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", False

        reference_list = [doc.metadata.get("source", f"doc_{i}") for i, doc in enumerate(filtered_docs)]
        treatment_by_pdf = {}

        for i, title in enumerate(reference_list):
            try:
                with open(f'./references/{title}.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    pdf_content = data.get("full_text", "")
            except FileNotFoundError:
                print(f"[ê²½ê³ ] íŒŒì¼ ì—†ìŒ: {title}.json â€” Solar LLMì´ ì‚¬ìš©ì ìƒíƒœë§Œ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨")
                continue  # PDFê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œëŠ” ê±´ë„ˆëœ€

            prompt = f"""
            ë„ˆëŠ” ì‘ê¸‰ì˜í•™ ë…¼ë¬¸ì´ë‚˜ ë¬¸ì„œë¥¼ ë¶„ì„í•´ì„œ 'ì¹˜ë£Œ(ì²˜ì¹˜)' ê´€ë ¨ ë‚´ìš©ë§Œ ìš”ì•½í•˜ëŠ” AIì•¼.
            ë‹¤ìŒì€ '{title}'ë¼ëŠ” ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì´ì•¼:
            \"\"\"{pdf_content}\"\"\" 
            ì´ ë¬¸ì„œì—ì„œ 'ì¹˜ë£Œ ë˜ëŠ” ì²˜ì¹˜ì— ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©'ë§Œ ë½‘ì•„ì„œ ìš”ì•½í•´ì¤˜.
            """
            response = solar_model.invoke([HumanMessage(content=prompt)])
            treatment_by_pdf[title] = response.content.strip()

        # PDF ê¸°ë°˜ ìš”ì•½ì´ í•˜ë‚˜ë„ ì—†ì„ ê²½ìš°, ì§ì ‘ ìƒíƒœ ê¸°ë°˜ ì‘ë‹µ
        if not treatment_by_pdf:
            fallback_prompt = f"""
            ë‹¤ìŒì€ í˜„ì¬ ì‘ê¸‰í™˜ìì˜ ìƒíƒœì•¼:
            \"\"\"{current_state}\"\"\" 

            ìœ„ ìƒíƒœë¥¼ ë³´ê³  ì–´ë–¤ ì²˜ì¹˜ë‚˜ ì¹˜ë£Œë¥¼ í•´ì•¼ í• ì§€ ì„¤ëª…í•´ì¤˜.
            ì¶”ê°€ë¡œ ì•Œì•„ì•¼ í•  ì •ë³´ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒë„ êµ¬ì²´ì ìœ¼ë¡œ ë§í•´ì¤˜.
            """
            fallback_response = solar_model.invoke([HumanMessage(content=fallback_prompt)])
            output = fallback_response.content.strip()
            additional_info_needed = not (
                "ì¶”ê°€ ì •ë³´ í•„ìš” ì—†ìŒ" in output or "ë” í•„ìš”í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤" in output
            )
            return output, additional_info_needed

        # ì •ìƒì ìœ¼ë¡œ ì¹˜ë£Œ ìš”ì•½ëœ ê²½ìš°
        all_treatments = "\n\n".join([
            f"[{src}]\n{summary}" for src, summary in treatment_by_pdf.items()
        ])
        reasoning_prompt = f"""
        [ì‹œìŠ¤í…œ ì—­í•  ë° ì¶œë ¥ ì§€ì¹¨]
        ë„ˆëŠ” ì‘ê¸‰ìƒí™©ì„ ë‹¤ë£¨ëŠ” ì˜ë£Œ AI ìƒë‹´ì›ì´ë‹¤. ë°˜ë“œì‹œ ì•„ë˜ 4í„´ êµ¬ì¡°ë¡œ ëŒ€ë‹µí•´ë¼:

        1. ì²« ë²ˆì§¸ í„´: ì‚¬ëŒì´ ì–´ë–¤ ë§ì„ í•˜ë©´, AIëŠ” ë°˜ë“œì‹œ í™˜ìì˜ ì„±ë³„, ë‚˜ì´, ê°„ë‹¨í•œ ìƒí™©, ì˜ì‹ ì—¬ë¶€ ë“±ì„ ë¬»ëŠ”ë‹¤.

        2. ë‘ ë²ˆì§¸ í„´: ì‚¬ëŒì´ ëŒ€ë‹µí•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì ì ˆí•œ ì‘ê¸‰ì¡°ì¹˜ë¥¼ ì œì‹œí•˜ê³  ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ë¥¼ ìš”ì²­í•œë‹¤.

        3. ì„¸ ë²ˆì§¸ í„´: ì¶”ê°€ ì •ë³´ë¥¼ í†µí•´ ì˜ì‹¬ ì¦ìƒê³¼ ì¹˜ë£Œ ë°©í–¥ì„ ì œì‹œí•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ í™•ì¸í•  ì •ë³´ë¥¼ ìš”ì²­í•œë‹¤.

        4. ë„¤ ë²ˆì§¸ í„´: ìµœì¢… ì‘ë‹µì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ì²˜ì¹˜ë¥¼ ì œì•ˆí•˜ê³ , ë³‘ì› ì „ì†¡ í•„ìš” ì—¬ë¶€ë¥¼ ì„¤ëª…í•˜ë©° ëŒ€í™”ë¥¼ ì¢…ë£Œí•œë‹¤.

        ê° ì‘ë‹µì€ ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ì¶œë ¥í•˜ë¼:
        - ë°˜ë“œì‹œ `1.`, `2.`, `3.`, `4.`ë¡œ ì¤„ ë²ˆí˜¸ë¥¼ ë¶™ì¸ë‹¤.
        - ê° ì¤„ì€ ì¤„ë°”ê¿ˆì„ í¬í•¨í•´ **í•œ ì¤„ì”© ë„ì›Œì„œ ì¶œë ¥**í•˜ë¼.
        - ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°ì—ë„ ë¹ˆì¹¸ ì—†ì´ ë„¤ ì¤„ ëª¨ë‘ ì¶œë ¥í•˜ë¼.
        - ì˜ë£Œ ìš©ì–´ëŠ” ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ê³ , ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±í•˜ë¼.

        ---

        [ì°¸ê³  ë¬¸ì„œ ìš”ì•½]
        ë‹¤ìŒì€ ì—¬ëŸ¬ ì‘ê¸‰ì˜í•™ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì¹˜ë£Œ ê´€ë ¨ ìš”ì•½ ì •ë³´ì•¼:
        {all_treatments}

        [í™˜ì ìƒíƒœ ì…ë ¥]
        í˜„ì¬ ì‘ê¸‰í™˜ìì˜ ìƒíƒœëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
        \"\"\"{current_state}\"\"\"

        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™˜ìì—ê²Œ ì–´ë–¤ ì²˜ì¹˜ë¥¼ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê³ , ì¶”ê°€ë¡œ ì•Œì•„ì•¼ í•  ì •ë³´ê°€ ìˆë‹¤ë©´ ëª…í™•í•˜ê²Œ ì•Œë ¤ì¤˜.  
        **ë§Œì•½ ì¶”ê°€ ì •ë³´ê°€ í•„ìš” ì—†ë‹¤ë©´ ë°˜ë“œì‹œ `"ì¶”ê°€ ì •ë³´ í•„ìš” ì—†ìŒ"`ì´ë¼ê³  ëª…ì‹œí•´.**
        """
        final_response = solar_model.invoke([HumanMessage(content=reasoning_prompt)])
        output = final_response.content.strip()

        additional_info_needed = not (
            "ì¶”ê°€ ì •ë³´ í•„ìš” ì—†ìŒ" in output or "ë” í•„ìš”í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤" in output
        )

        return output, additional_info_needed

    except Exception as e:
        print(f"[RAG ì²˜ë¦¬ ì˜¤ë¥˜]: {e}")
        return "ì¹˜ë£Œ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", False


def generate_medical_brief(conversation, solar_model):
    dialogue = ""
    for turn in conversation:
        role = "ğŸ‘¨â€âš•ï¸ì‘ê¸‰êµ¬ì¡°ì‚¬" if turn["role"] == "user" else "ğŸ¤–AI"
        dialogue += f"{role}: {turn['content']}\n"

    prompt = f"""
    ë„ˆëŠ” ë³‘ì›ì— í™˜ì ìƒíƒœë¥¼ ì •ë¦¬í•´ì„œ ì „ë‹¬í•˜ëŠ” ì‘ê¸‰êµ¬ì¡°ì‚¬ AIì•¼.

    ì•„ë˜ëŠ” í˜„ì¥ì—ì„œ ì‘ê¸‰êµ¬ì¡°ì‚¬ì™€ AI ê°„ì˜ ì‹¤ì œ ëŒ€í™” ë¡œê·¸ì•¼:
    -------------------------------
    {dialogue}
    -------------------------------

    ìœ„ ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³‘ì›ì— ì „ë‹¬í•  ê³µì‹ ë¸Œë¦¬í•‘ ë¬¸ì„œë¥¼ ì‘ì„±í•´ì¤˜.
    í¬ë§·ì€ ë‹¤ìŒê³¼ ê°™ì•„:

    ---
    ğŸ§¾ [í™˜ì ìƒíƒœ ìš”ì•½]
    - (ì²˜ìŒ ë³´ê³ ëœ ìƒíƒœ ìš”ì•½)

    ğŸ’‰ [AIê°€ ì œì•ˆí•œ ì¹˜ë£Œ ë° ì²˜ì¹˜ ìš”ì•½]
    - (ì¹˜ë£Œ ìš”ì•½ ìš”ì )

    â— [í˜„ì¥ì—ì„œ ìˆ˜ì§‘ëœ ì¶”ê°€ ì •ë³´]
    - (êµ¬ì¡°ì‚¬ê°€ ë‚˜ì¤‘ì— ì…ë ¥í•œ ë³´ì™„ ì •ë³´)

    ğŸ“Œ [ìµœì¢… íŒë‹¨ ë˜ëŠ” ì´ì†¡ ì§€ì‹œ ìš”ì•½]
    - (AIê°€ ìµœì¢…ì ìœ¼ë¡œ ë‚´ë¦° íŒë‹¨ ë° ë³‘ì› ì´ì†¡ í•„ìš” ì—¬ë¶€)
    ---
    """
    response = solar_model.invoke([HumanMessage(content=prompt)])
    return response.content.strip()

@app.post("/generate_medical_brief")
async def generate_medical_brief(request: Request):
    try:
        data = await request.json()
        conversation = data.get("conversation", [])
        
        if not conversation:
            raise HTTPException(status_code=400, detail="ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëŒ€í™” ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        conversation_text = "\n".join([
            f"{'ì‘ê¸‰êµ¬ì¡°ì‚¬' if msg['role'] == 'user' else 'AI'}: {msg['content']}"
            for msg in conversation
        ])
        
        print("ëŒ€í™” ë‚´ìš©:", conversation_text)  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        # LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""ë„ˆëŠ” ì‘ê¸‰ìƒí™©ì„ ë¶„ì„í•˜ê³  ì˜ë£Œ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ëŠ” AI ì˜ë£Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì˜ë£Œ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©:
{conversation_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:

1. í™˜ì ìƒíƒœ ìš”ì•½
- í™˜ìì˜ ê¸°ë³¸ ì •ë³´ (ë‚˜ì´, ì„±ë³„)
- í˜„ì¬ ìƒíƒœì™€ ì£¼ìš” ì¦ìƒ
- ì˜ì‹ ìƒíƒœ
- ìƒì²´ì§•í›„ (ì•Œë ¤ì§„ ê²½ìš°)

2. ì¹˜ë£Œ ë° ì²˜ì¹˜ ìš”ì•½
- í˜„ì¬ê¹Œì§€ ì‹œí–‰ëœ ì²˜ì¹˜
- ì¶”ê°€ë¡œ í•„ìš”í•œ ì¹˜ë£Œ
- ì•½ë¬¼ íˆ¬ì—¬ ë‚´ì—­ (ìˆëŠ” ê²½ìš°)

3. ì¶”ê°€ ì •ë³´
- ì•Œë ˆë¥´ê¸°ë‚˜ ê¸°ì €ì§ˆí™˜
- ì‚¬ê³ /ì¦ìƒ ë°œìƒ ì‹œì 
- íŠ¹ì´ì‚¬í•­ì´ë‚˜ ì£¼ì˜ì 

4. ìµœì¢… íŒë‹¨
- ì˜ì‹¬ë˜ëŠ” ì§ˆí™˜/ìƒíƒœ
- ì‘ê¸‰ë„ í‰ê°€
- ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

ê° ì„¹ì…˜ì€ ë°˜ë“œì‹œ bullet point(-)ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª©ë“¤ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
ê° í•­ëª©ì€ êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ëª¨ë“  ì„¹ì…˜ì— ëŒ€í•´ ìµœì†Œí•œ í•˜ë‚˜ ì´ìƒì˜ í•­ëª©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        # Solar LLM í˜¸ì¶œ
        response = solar_model.invoke([HumanMessage(content=prompt)])
        content = response.content.strip()
        
        print("LLM ì‘ë‹µ:", content)  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        return {"content": content}
        
    except Exception as e:
        print(f"ë¸Œë¦¬í•‘ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/send_to_hospital")
async def send_to_hospital(briefing_data: dict):
    try:
        # TODO: ì‹¤ì œ ë³‘ì› ì „ì†¡ ë¡œì§ êµ¬í˜„
        # í˜„ì¬ëŠ” ì„±ê³µ ì‘ë‹µë§Œ ë°˜í™˜
        return {"status": "success", "message": "ë³‘ì›ìœ¼ë¡œ ë¸Œë¦¬í•‘ì´ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"ë³‘ì› ì „ì†¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
        )
