[
  {
    "full_text": "TITLE: Application of Machine Learning for Patients With Cardiac Arrest: Systematic Review and Meta-Analysis ABSTRACT: BackgroundCurrently, there is a lack of effective early assessment tools for predicting the onset and development of cardiac arrest (CA). With the increasing attention of clinical researchers on machine learning (ML), some researchers have developed ML models for predicting the occurrence and prognosis of CA, with certain models appearing to outperform traditional scoring tools. However, these models still lack systematic evidence to substantiate their efficacy.ObjectiveThis systematic review and meta-analysis was conducted to evaluate the prediction value of ML in CA for occurrence, good neurological prognosis, mortality, and the return of spontaneous circulation (ROSC), thereby providing evidence-based support for the development and refinement of applicable clinical tools.MethodsPubMed, Embase, the Cochrane Library, and Web of Science were systematically searched from their establishment until May 17, 2024. The risk of bias in all prediction models was assessed using the Prediction Model Risk of Bias Assessment Tool.ResultsIn total, 93 studies were selected, encompassing 5,729,721 in-hospital and out-of-hospital patients. The meta-analysis revealed that, for predicting CA, the pooled C-index, sensitivity, and specificity derived from the imbalanced validation dataset were 0.90 (95% CI 0.87-0.93), 0.83 (95% CI 0.79-0.87), and 0.93 (95% CI 0.88-0.96), respectively. On the basis of the balanced validation dataset, the pooled C-index, sensitivity, and specificity were 0.88 (95% CI 0.86-0.90), 0.72 (95% CI 0.49-0.95), and 0.79 (95% CI 0.68-0.91), respectively. For predicting the good cerebral performance category score 1 to 2, the pooled C-index, sensitivity, and specificity based on the validation dataset were 0.86 (95% CI 0.85-0.87), 0.72 (95% CI 0.61-0.81), and 0.79 (95% CI 0.66-0.88), respectively. For predicting CA mortality, the pooled C-index, sensitivity, and specificity based on the validation dataset were 0.85 (95% CI 0.82-0.87), 0.83 (95% CI 0.79-0.87), and 0.79 (95% CI 0.74-0.83), respectively. For predicting ROSC, the pooled C-index, sensitivity, and specificity based on the validation dataset were 0.77 (95% CI 0.74-0.80), 0.53 (95% CI 0.31-0.74), and 0.88 (95% CI 0.71-0.96), respectively. In predicting CA, the most significant modeling variables were respiratory rate, blood pressure, age, and temperature. In predicting a good cerebral performance category score 1 to 2, the most significant modeling variables in the in-hospital CA group were rhythm (shockable or nonshockable), age, medication use, and gender; the most significant modeling variables in the out-of-hospital CA group were age, rhythm (shockable or nonshockable), medication use, and ROSC.ConclusionsML represents a currently promising approach for predicting the occurrence and outcomes of CA. Therefore, in future research on CA, we may attempt to"
  },
  {
    "full_text": "in future research on CA, we may attempt to systematically update traditional scoring tools based on the superior performance of ML in specific outcomes, achieving artificial intelligence–driven enhancements.Trial RegistrationPROSPERO International Prospective Register of Systematic Reviews CRD42024518949; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=518949 BODY: IntroductionBackgroundCardiac arrest (CA) remains a critical challenge in contemporary medicine, characterized by a dismally low survival rate and poor prognosis, and, therefore, has garnered global attention [1]. CA can be classified by the occurrence location into in-hospital CA (IHCA) and out-of-hospital CA (OHCA). Despite advancements in cardiopulmonary resuscitation techniques, global registry data indicate that the incidence and survival rates of CA have not significantly improved. The incidence of IHCA in the United States increased to 900 to 1000 per 100,000 hospitalized patients between 2008 and 2017, compared to 160 per 100,000 in the United Kingdom from 2011 to 2013 and 840 per 100,000 in China as of 2020 [2-4]. Meanwhile, the estimated averages of incidence of OHCA under emergency medical services (EMS) in North America, Asia, and Europe from 2010 to 2020 were 47.3, 45.9, and 40.6 per 100,000 people, respectively. The estimated averages of the survival rates of IHCA from 2010 to 2020 were 25% in the United States, 18% in the United Kingdom, and only 9.4% in China. For OHCA, the estimated averages of the survival rates during this same period were 10% to 12% in the United States, 8% in Europe, and just 3.6% in Asia [2,5]. These low survival rates also impose significant economic burdens on nations. According to relevant reviews, the cost-effectiveness threshold for CA ranged from US $20,000 to US $150,000 per quality-adjusted life year, with each life saved potentially reducing costs by US $19,000 to US $71,000 per case [6].Although efforts to establish CA centers independently began in various regions of the United States as early as 2000 to 2010 [7] and Germany initiated CA center certification throughout the country [8] in August 2019 aiming to provide evidence-based, bundled care to improve CA survival rates, CA remains a formidable clinical challenge. If resuscitation is not timely, the patient may lose consciousness within approximately 10 seconds, with irreversible hypoxic-ischemic brain injury occurring within 4 minutes [9], and if resuscitation is delayed beyond 10 minutes, survival is practically impossible [10,11]. Thus, early prediction and identification of CA are critical factors in preventing death and poor outcomes and represent a major challenge that requires urgent clinical attention.ObjectivesHowever, there is a scarcity of efficient, internationally recognized, and universally accepted assessment tools for early prediction and identification of CA risk and adverse outcomes. In recent years, with the rapid advancement of artificial"
  },
  {
    "full_text": "years, with the rapid advancement of artificial intelligence (AI), many researchers have used machine learning (ML) to address clinical challenges. Commonly used ML approaches can be broadly categorized into supervised and unsupervised learning. In the context of supervised ML, clinical predictors can be incorporated into various models. In these models, their parameters are adjusted based on outcome variables to generate predictions regarding the probability of positive event occurrence [12]. It is now common to see ML being used to predict disease progression and even to diagnose and treat complex diseases effectively. For instance, in 2019, several authors, including Hatib et al [13] and Wijnberge et al [14], successfully predicted intraoperative hypotensive events using ML, leading to the clinical translation of these models into products that significantly enhanced patient safety during surgical anesthesia [15]. By 2023, some researchers had similarly affirmed the substantial potential of ML models in cancer detection, prognosis, and treatment, recognizing their exciting discoveries and contributions to advancing medical practice [16]. The aforementioned studies were based on supervised ML and the extensive use of interpretable clinical features to construct predictive models and simultaneously demonstrate the promising predictive performance of ML in clinical events across various fields. In this context, some researchers have also developed different ML models for risk prediction in CA. Recent reviews by Sem et al [17] and Chen et al [18] indicate that ML appears to exhibit high accuracy in both the management and risk prediction of CA. However, these reviews do not quantitatively synthesize the results of ML models, which significantly limits our ability to interpret the specific value of various ML models in CA applications and the selection of appropriate models. Therefore, we conducted this systematic review and meta-analysis to review the predictive performance of ML for the occurrence of CA, good neurological prognosis after CA, mortality, and the return of spontaneous circulation (ROSC) after CA to provide evidence-based guidance for the development and updating of simple prediction tools with high accuracy and direct access to results.MethodsStudy RegistrationThis study was conducted in adherence to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines and prospectively registered with PROSPERO (International Prospective Register of Systematic Reviews; ID CRD42024518949). The detailed PRISMA checklist is presented inMultimedia Appendix 1.Eligibility CriteriaDetailed inclusion and exclusion criteria were defined to screen the original studies relevant to our systematic review from the retrieved literature (Textbox 1).Inclusion and exclusion criteria for original studies.Inclusion criteriaStudy type: the included studies must be case-control, cohort, nested case-control, case-cohort, or"
  },
  {
    "full_text": "cohort, nested case-control, case-cohort, or cross-sectional studies.Model construction: although some studies, due to limited sample sizes, lacked independent external validation, we could not dismiss their contributions. In our analysis, it was necessary to synthesize results from the training and validation sets to assess the presence of severe overfitting. Therefore, those with no external validation were also included.Outcomes: studies that comprehensively constructed machine learning (ML) models for cardiac arrest (CA) occurrence prediction or clinical outcomes following CA were selected.Language: we included original studies in English.Exclusion criteriaStudy type: studies categorized as meta-analyses, reviews, guidelines, expert opinions, or conference abstracts and not fully peer reviewed and published were removed.Model construction: studies with only risk factor analysis but no construction of a complete ML model were excluded, those with a limited number of samples (<20) were not included, and those only focusing on the accuracy of univariate predictors were removed.Outcomes: in existing ML studies, model performance was assessed using the receiver operating characteristic curve, C-statistic, sensitivity, specificity, accuracy, recall, precision, confusion matrix, orF1-score. However, a few original studies that lacked at least one of these metrics and, therefore, did not evaluate model performance adequately were excluded.Language: non–English-language original studies were excluded.Data Sources and Search StrategyA systematic search of the PubMed, Embase, Cochrane Library, and Web of Science databases was carried out from their inception to May 17, 2024. The search strategy involved controlled vocabulary and free-text terms, with no restrictions on geographical location or publication year. The detailed search strategy is presented inMultimedia Appendices 2-5.Study Selection and Data ExtractionThe retrieved studies were imported into EndNote X9 (Clarivate Analytics). Their titles and abstracts were reviewed. After the exclusion of duplicates, the preliminary eligible original studies were selected and their full texts downloaded for determining the final inclusion. An electronic spreadsheet was prepared to extract the following information: first author, publication year, author’s country, study type, patient source, prediction events, data balance, location of CA occurrence, number of cases with study events, total number of cases, number of cases in the training and validation sets, method of validation set generation, missing data–handling methods, and types of models used. Study selection and data extraction were independently conducted by 2 researchers. Disagreements were discussed and resolved with a third author.Risk of Bias in the StudiesThe Prediction Model Risk of Bias Assessment Tool (PROBAST) was used to assess the risk of bias in all the included original studies. PROBAST comprises several questions across 4"
  },
  {
    "full_text": "PROBAST comprises several questions across 4 domains—participant, predictor, outcome, and statistical analysis—which reflect the overall risk of bias and applicability. These domains consist of 2, 3, 6, and 9 questions, respectively, each with 3 possible answers (YesorProbablyyes,NoorProbably no, andNo information). A domain was classified as high risk if any question was answered withNoorProbably no. Conversely, a domain was regarded as low risk only if every question was answered withYesorProbably yes. The overall risk of bias was assessed as low when all domains were deemed to be low risk. When at least one domain was high risk, the overall risk of bias was rated as high. In total, 2 authors independently assessed the risk of bias using PROBAST and cross-checked their findings. Any discrepancies were addressed by consulting with a third author to reach agreement.OutcomesThe primary outcome measure was the C-index, which reflects the predictive ability of ML models for IHCA and OHCA. However, we found that the C-index might not have accurately described the predictive performance of ML for positive events, particularly in models built on severely imbalanced data, as these original studies often suffered from such imbalance. This limitation was evident in predicting the occurrence of IHCA and OHCA, the good cerebral performance category score 1 to 2 (CPC 1-2), mortality, and ROSC. Therefore, in addition to the C-index, our primary outcome measures encompassed sensitivity and specificity. Our secondary outcome was the frequency of variables used in the ML models.Synthesis MethodsA meta-analysis of the C-index, a measure of the general accuracy of ML models, was carried out. When the 95% CI and SE for the C-index were missing in some studies, the SE was estimated based on the study by Debray et al [19]. Due to the differences in the included variables and inconsistent parameters among the ML models, random-effects models were prioritized in the meta-analysis of the C-index. In addition, a meta-analysis on sensitivity and specificity was conducted through a bivariate mixed-effects model based on diagnostic 2 × 2 tables. However, most original studies did not report these tables. In such cases, we used the following methods to calculate the 2 × 2 tables: (1) calculation based on sensitivity, specificity, precision, and case numbers; and (2) calculation based on the best Youden index to extract sensitivity and specificity, followed by case number integration. Nevertheless, this method allowed for meta-analysis only when there were ≥4 models. For <4 models, we presented the range of sensitivity and specificity. Our meta-analysis was conducted in R (version 4.2.0; R Foundation for Statistical Computing).ResultsStudy SelectionA total of 1270 articles were obtained from databases, with 599 (47.17%) being duplicates. Among these 599 duplicates, 471 (78.6%) were found to be duplicates via software, and 128 (21.4%) were manually identified as duplicates."
  },
  {
    "full_text": "(21.4%) were manually identified as duplicates. After the elimination of duplicates, 671 articles were screened by title and abstract, with 169 (25.2%) being selected for full-text review. After the exclusion of conference abstracts published in full text without peer review (19/169, 11.2%), studies with risk factor analyses but no complete ML models (22/169, 13%), studies lacking outcome indicators (28/169, 16.6%), and studies with severe statistical errors (7/169, 4.1%), a total of 93 articles were included finally. The detailed process is illustrated inFigure 1.Figure 1The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow diagram for study selection.Study CharacteristicsThe 93 selected studies were published between 2011 and 2024, covering 14 countries, primarily South Korea, China, Japan, the United States, and Singapore. Among the 93 studies, there were 23 (25%) prospective cohort studies and 3 (3%) case-control studies, with the remainder (67/93, 72%) being retrospective cohort studies. Data for 26% (24/93) of the studies were sourced from multiple centers, whereas 37% (34/93) of the studies used data from registry databases and the rest (35/93, 38%) were single-center studies. In 30% (28/93) of the studies, the predicted outcome was the occurrence of CA. In 42 studies, the predicted outcome was the neurological prognosis of patients with CA, with 10 (24%) studies focused on patients with IHCA and the remainder (n=32, 76%) focused on patients with OHCA. In 27% (25/93) of the studies, the predicted outcome was CA mortality, and in 12% (11/93) of the studies, the predicted outcome was ROSC in patients with CA. The 93 studies collectively encompassed a total of 5,729,721 cases, including 1,737,085 OHCA cases and 3,992,636 IHCA cases. Regarding the predictive models constructed, 81% (75/93) of the studies had independent validation sets, but only 27% (25/93) used external validation, primarily using k-fold cross-validation and random-sampling internal validation methods. A total of 34% (32/93) of the studies described methods to prevent data overfitting, mainly through cross-validation. In total, 17 types of ML models were involved, with logistic regression (LR), random forest (RF), deep learning, and decision trees (DTs) being the most prominent. In addition, these studies validated several previously established scoring tools, including the Cardiac Arrest Neurological Prognosis score, distance scoring system, Emergency Department In-Hospital Cardiac Arrest Score, FACTOR score, Modified Early Warning Score, National Early Warning Score, National Early Warning Score 2, OHCA score, proposed scoring system, Rapid Emergency Medicine Score, Simplified Acute Physiology Score II, Cardiac Arrest Hospital Prognosis score, and ROSC after CA score. The details of the included studies are shown inTables 1and2.Table 1Characteristics of the included studies.StudyYear of publicationCountry of first authorStudy type"
  },
  {
    "full_text": "of publicationCountry of first authorStudy type (case-control, cohort study [retrospective or prospective], nested cohort study, or case-cohort study)Patient sources (single center, multicenter, or registration database)Predictive eventsWang et al [20]2024Taiwan, ChinaRetrospective cohort studyMulticenterCardiac arrestRaheem et al [21]2024PakistanRetrospective cohort studySingle centerCardiac arrestAmacher et al [22]2024SwitzerlandProspective cohort studySingle centerIn-hospital mortality and CPC 3-5aCho et al [23]2024Republic of KoreaRetrospective cohort studySingle centerCardiac arrestShin et al [24]2024Republic of KoreaRetrospective cohort studySingle centerCardiac arrestDing et al [25]2024ChinaProspective cohort studySingle centerCPC 1-2band in-hospital mortalityNishioka et al [26]2024JapanRetrospective cohort studyMulticenterCPC 3-5Kajino et al [27]2024JapanRetrospective cohort studyRegistration databaseCPC 1-2Pham et al [28]2024United StatesProspective cohort studyMulticenterCardiac arrestRahadian et al [29]2024JapanProspective cohort studyRegistration databaseVFcor VTdWang et al [30]2024Taiwan, ChinaRetrospective cohort studyRegistration databaseCardiac arrestTsai et al [31]2024Taiwan, ChinaRetrospective cohort studySingle centerCPC 1-2Schweiger et al [32]2024SwitzerlandProspective cohort studySingle centerIn-hospital mortalityCaputo et al [33]2024SwitzerlandProspective cohort studyRegistration databaseROSCeLu et al [34]2023Taiwan, ChinaRetrospective cohort studySingle centerCardiac arrestDünser et al [35]2023AustriaRetrospective cohort studySingle centerNROSCfand CPC 3-5Bang et al [36]2023Republic of KoreaRetrospective cohort studyMulticenterIn-hospital mortalityZhang et al [37]2023ChinaRetrospective cohort studyMulticenterIn-hospital mortalityLi and Xing [38]2023ChinaRetrospective cohort studySingle centerCPC 3-5 and NROSCDing et al [39]2023ChinaRetrospective cohort studySingle centerCardiac arrestUehara et al [40]2023JapanProspective cohort studyMulticenterCPC 1-2Shin et al [41]2023Republic of KoreaRetrospective cohort studyRegistration databaseCPC 1-2 and ROSCKawai et al [42]2023JapanRetrospective cohort studySingle centerCPC 3-5Imamura et al [43]2023JapanRetrospective cohort studyMulticenter30-day mortalityHessulf et al [44]2023SwedenRetrospective cohort studyMulticenter30-day survivalYoon et al [45]2023Republic of KoreaRetrospective cohort studySingle centerCPC 3-5Chang et al [46]2023Republic of KoreaRetrospective cohort studyRegistration databaseROSC, survival to discharge, and CPC 1-2Wang et al [47]2023ChinaRetrospective cohort studyMulticenterROSCShinada et al [48]2023JapanRetrospective cohort studyRegistration databaseCPC 1-2Xu et al [49]2022ChinaCase-control studySingle centerCardiac arrestTsai et al [50]2022Taiwan, ChinaRetrospective cohort studyRegistration databaseCardiac arrestTang et al [51]2022ChinaRetrospective cohort studyRegistration databaseCardiac arrestKim et al [52]2022Republic of KoreaRetrospective cohort"
  },
  {
    "full_text": "al [52]2022Republic of KoreaRetrospective cohort studyRegistration databaseCardiac arrestChae et al [53]2022Republic of KoreaRetrospective cohort studySingle centerCardiac arrestSun et al [54]2022Taiwan, ChinaRetrospective cohort studySingle centerCardiac arrestWong et al [55]2022SingaporeProspective cohort studyMulticenterSurvival to dischargeTran et al [56]2022United StatesProspective cohort studyRegistration databaseIn-hospital mortalityRajendram et al [57]2022SingaporeRetrospective cohort studyMulticenterSurvival to discharge and CPC 1-2Rafi et al [58]2022FranceRetrospective cohort studySingle centerCardiac arrestLiu et al [59]2022SingaporeRetrospective cohort studyRegistration databaseROSCLin et al [60]2022Taiwan, ChinaRetrospective cohort studyRegistration databaseCPC 1-2Kawai et al [61]2022JapanRetrospective cohort studyMulticenterCPC 1-2Itagaki et al [62]2022JapanRetrospective cohort studySingle centerBrain deathHarris et al [63]2022United StatesRetrospective cohort studyRegistration databasePrehospital ROSC in pediatric OHCAgHarford et al [64]2022United StatesRetrospective cohort studyRegistration databaseCPC 1-2Harford et al [65]2022United StatesRetrospective cohort studyRegistration databaseCPC 1-2Chung et al [66]2021Taiwan, ChinaRetrospective cohort studySingle centerCPC 1-2Chi et al [67]2021Taiwan, ChinaRetrospective cohort studyRegistration databaseIn-hospital mortalityWang et al [68]2021ChinaRetrospective cohort studyMulticenterCPC 1-2Bae et al [69]2021Republic of KoreaRetrospective cohort studySingle centerCPC 3-5Mueller et al [70]2021AustriaProspective cohort studySingle centerCPC 1-2Lee et al [71]2021Republic of KoreaRetrospective cohort studyMulticenterCardiac arrestLim et al [72]2021Republic of KoreaProspective cohort studyRegistration databaseCPC 1-2Lo and Siu [73]2021Hong Kong, ChinaRetrospective cohort studyRegistration databaseROSCLonsain et al [74]2021BelgiumRetrospective cohort studySingle center24-hour survivalNishioka et al [75]2021JapanProspective cohort studyRegistration databaseCPC 1-2Beom et al [76]2021Republic of KoreaProspective cohort studyMulticenterSurvival to discharge and CPC 1-2Cheng et al [77]2021Taiwan, ChinaRetrospective cohort studySingle centerCPC 1-2Kim et al [78]2021Republic of KoreaRetrospective cohort studyRegistration databaseSurvival to discharge and CPC 1-2Seo et al [79]2021Republic of KoreaProspective cohort studyRegistration databaseCPC 1-2Song et al [80]2021Republic of KoreaRetrospective cohort studySingle centerCPC 3-5Sun et al [81]2021Hong Kong, ChinaRetrospective cohort studyMulticenterROSCYoun et al [82]2021Republic of KoreaProspective cohort studyMulticenterSignificant coronary artery disease among survivors of OHCA without STEhHeo et al [83]2021Republic of KoreaProspective cohort studyMulticenterCPC 3-5Wang et al [84]2020ChinaRetrospective cohort studyRegistration databaseCPC 1-2Hong et al [85]2020Republic of KoreaRetrospective cohort studySingle centerCardiac arrestCho et al"
  },
  {
    "full_text": "cohort studySingle centerCardiac arrestCho et al [86]2020Republic of KoreaRetrospective cohort studySingle centerCardiac arrestHirano et al [87]2020JapanRetrospective cohort studyRegistration databaseDeath at 1 month or survival with poor neurological function (CPC 3-5) and 30-day mortalityOkada et al [88]2020JapanProspective cohort studyRegistration databaseCPC 1-2Liu et al [89]2020SingaporeRetrospective cohort studyRegistration databaseROSCElola et al [90]2020SpainRetrospective cohort studyRegistration databaseCardiac arrestHsieh et al [91]2020Taiwan, ChinaRetrospective cohort studyRegistration databaseCardiac arrestBaldi et al [92]2020ItalyProspective cohort studyMulticenterSurvival to hospital admissionLi et al [93]2019ChinaCase-control studyMulticenterCardiac arrestSrivilaithon et al [94]2019ThailandCase-control studySingle centerCardiac arrestLee et al [95]2019Republic of KoreaRetrospective cohort studySingle centerCPC 3-5Liu et al [96]2019Taiwan, ChinaRetrospective cohort studySingle centerCardiac arrestJang et al [97]2019Republic of KoreaRetrospective cohort studySingle centerCardiac arrestSeki et al [98]2019JapanProspective cohort studyMulticenter1-year survivalPark et al [99]2019Republic of KoreaRetrospective cohort studyRegistration databaseCPC 1-2Kwon et al [100]2019Republic of KoreaRetrospective cohort studyRegistration databaseCPC 1-2 and survival to dischargeKong et al [101]2019Republic of KoreaProspective cohort studyMulticenterCPC 1-2 and survival to dischargeHarford et al [102]2019United StatesRetrospective cohort studyRegistration databaseCPC 1-2Kwon et al [103]2018Republic of KoreaRetrospective cohort studyMulticenterCardiac arrest and in-hospital mortalityChang et al [104]2018Taiwan, ChinaRetrospective cohort studySingle centerCardiac arrestShin et al [105]2018Republic of KoreaRetrospective cohort studyMulticenterCPC 1-2Lee et al [106]2017Republic of KoreaRetrospective cohort studySingle centerSurvival to hospital dischargeLiu et al [107]2015SingaporeRetrospective cohort studySingle centerCardiac arrestGoto et al [108]2014JapanRetrospective cohort studyRegistration databaseCPC 1-2 and 30-day survivalGoto et al [109]2013JapanRetrospective cohort studyRegistration databaseCPC 1-2 and 30-day survivalHock Ong et al [110]2012SingaporeProspective cohort studySingle centerCardiac arrest and in-hospital mortalityHayakawa et al [111]2011JapanProspective cohort studyRegistration databaseCPC 1-2aCPC 3-5: poor cerebral performance category score 3 to 5.bCPC 1-2: good cerebral performance category score 1 to 2.cVF: ventricular fibrillation.dVT: ventricular tachycardia.eROSC: return of spontaneous circulation.fNROSC: non-ROSC.gOHCA: out-of-hospital cardiac arrest.hSTE: ST segment elevation.Table 2Analytical characteristics of the included studies.StudyBalance of data (balanced or unbalanced)Location of CAaNumber of cases of studied eventsTotal number of casesNumber of cases in the training setGeneration of validation setNumber of cases in"
  },
  {
    "full_text": "setGeneration of validation setNumber of cases in the validation setHandling method for missing valuesModel typeWang et al [20]UnbalancedIn hospital474224,413182,716External validation41,697DeletionLogistic regression, National Early Warning Score, and Modified Early Warning ScoreRaheem et al [21]UnbalancedIn hospital548397,35377,886Internal validation19,467DeletionArtificial neural network, random forest, and logistic regressionAmacher et al [22]BalancedIn hospital and out of hospitalIHMb: 309; CPC 3-5c: 309713—d—713No processingOut-of-hospital CA score, the Cardiac Arrest Hospital Prognosis score, and logistic regressionCho et al [23]UnbalancedIn hospital22895,607—External validation95,607DeletionDeep learning, Modified Early Warning Score, and National Early Warning ScoreShin et al [24]UnbalancedIn hospital1981995970External validation1025DeletionDeep learning, logistic regression, random forest, and National Early Warning ScoreDing et al [25]BalancedIn hospitalCPC 1-2e: 20; IHM: 3053—Internal validation53DeletionLogistic regression and Cox regressionNishioka et al [26]BalancedOut of hospital648675873337External validation4250SupplementLogistic regressionKajino et al [27]UnbalancedOut of hospital11,411302,799149,425Internal validation153,374DeletionDeep learningPham et al [28]BalancedOut of hospital210434231External validation (multicenter)203—Logistic regressionRahadian et al [29]UnbalancedOut of hospital86020,71317,162—3551ImputationLogistic regression, LASSOf, and random forestWang et al [30]UnbalancedOut of hospital8448,37132,244—16,127—Logistic regressionTsai et al [31]BalancedOut of hospitalCPC 1-2: 127443265Internal validation178—Logistic regressionSchweiger et al [32]BalancedOut of hospital120291138Internal validation153SupplementFACTOR scoreCaputo et al [33]UnbalancedOut of hospital271912,577—Internal validation12,577—Logistic regressionLu et al [34]UnbalancedEmergency department636316,465237,349Random sampling79,116SupplementLogistic regression, random forest, National Early Warning Score 2, and XGBoostgDünser et al [35]BalancedOperating rooms and departments outside the ICUhNROSCi: 390; CPC 3-5: 559630—Internal validation630No processingRandom forestBang et al [36]BalancedIn hospital4111133754Random sampling379DeletionLogistic regressionZhang et al [37]BalancedIn hospital495561561——DeletionLogistic regressionLi and Xing [38]BalancedIn hospitalNROSC: 564; CPC 3-5: 229851851Internal validation (bootstrap)—DeletionLogistic regressionDing et al [39]BalancedIn hospital176935922873Internal validation719DeletionSupport vector machine, random forest, XGBoost, decision tree, and logistic regressionUehara et al [40]UnbalancedOut of hospital7184224239Random sampling (1:1)4183DeletionLogistic regressionShin et al [41]UnbalancedOut of hospitalROSCj: 3095; CPC 1-2: 99016,992—Random sampling—DeletionK-nearest neighbor, decision tree, random forest, support vector machine, logistic regression, and deep learningKawai et al [42]BalancedOut of"
  },
  {
    "full_text": "and deep learningKawai et al [42]BalancedOut of hospital254321257Random sampling (8:2）64DeletionDeep learningImamura et al [43]BalancedOut of hospital172274194External validation (multicenter)80DeletionLogistic regressionHessulf et al [44]UnbalancedOut of hospital619155,61544,492Random sampling11,123AlgorithmXGBoostYoon et al [45]BalancedOut of hospital74131—External validation131DeletionLogistic regressionChang et al [46]UnbalancedOut of hospitalROSC: 11,996; STDk: 11,833; 30-day survival: 7760; CPC 1-2: 3673157,654157,654Internal validation—DeletionLightGBMlWang et al [47]UnbalancedOut of hospital15626852685Internal validation—DeletionLogistic regressionShinada et al [48]UnbalancedOut of hospital112853404286Internal validation1054DeletionNaïve BayesXu et al [49]BalancedEmergency department and out of hospital150600600——DeletionLogistic regressionTsai et al [50]UnbalancedEmergency department623325,502325,502——No processingLogistic regression, Modified Early Warning Score, and National Early Warning ScoreTang et al [51]BalancedICU107486—Internal validation486AlgorithmNational Early Warning Score, random forest, artificial neural network, and deep learningKim et al [52]UnbalancedEmergency department54311,350,6931,080,554Random sampling270,139DeletionLogistic regression, XGBoost, artificial neural network, and logistic regressionChae et al [53]UnbalancedIn hospital57334,452—Random sampling34,452SupplementDecision tree, random forest, logistic regression, and artificial neural networkSun et al [54]UnbalancedEmergency department240145,557—External validation145,557DeletionEmergency department, in-hospital CA score, Modified Early Warning Score, and Rapid Emergency Medicine ScoreWong et al [55]UnbalancedOut of hospital85547763582Random sampling1194DeletionRandom forestTran et al [56]BalancedOut of hospital99629992999Internal validation—DeletionLogistic regressionRajendram et al [57]UnbalancedOut of hospitalSTD: 3549; CPC 1-2: 175424,897—External validation24,897DeletionRandom forestRafi et al [58]BalancedOut of hospital410820—Internal validation820Supplement (algorithm)Logistic regression, random forest, and artificial neural networkLiu et al [59]UnbalancedOut of hospital12,729153,611119,477External validation (multicenter)34,134DeletionRandom forestLin et al [60]UnbalancedOut of hospital16035202816Random sampling (8:2)704DeletionDecision tree and random forestKawai et al [61]UnbalancedOut of hospital28682746620Internal validation (cross-validation)1654DeletionArtificial neural networkItagaki et al [62]BalancedOut of hospitalBDm: 77419—Internal validation (bootstrap)419DeletionLogistic regressionHarris et al [63]UnbalancedOut of hospital39917261381Random sampling345Supplement (algorithm)Logistic regression, random forest, and LightGBMHarford et al [64]UnbalancedOut of hospital3791798957Internal validation (cross-validation)241 and 600DeletionLightGBM, XGBoost, decision tree, random forest, k-nearest neighbor, logistic regression, and deep"
  },
  {
    "full_text": "k-nearest neighbor, logistic regression, and deep learningHarford et al [65]UnbalancedOut of hospital67095955750Random sampling1445 and 2400DeletionDeep learningChung et al [66]UnbalancedIn hospital94796637Random sampling159No processingArtificial neural networkChi et al [67]BalancedIn hospital87,311168,693168,693——DeletionHVecnWang et al [68]UnbalancedIn hospital4615980External validation (multicenter)79DeletionCANPoscoreBae et al [69]BalancedIn hospital643982671External validation (prospective)311DeletionLogistic regressionMueller et al [70]BalancedIn hospital223475475——DeletionLogistic regressionLee et al [71]UnbalancedIn hospital425332,371173,368External validation (multicenter)159,003AlgorithmDeep learning and Modified Early Warning ScoreLim et al [72]Unbalanced—89282404712External validation (prospective)3528DeletionLogistic regressionLo and Siu [73]UnbalancedOut of hospital278781576525Random sampling1632DeletionLogistic regression, random forest, and artificial neural networkLonsain et al [74]BalancedOut of hospital168192192Internal validation——Logistic regressionMueller et al [70]BalancedOut of hospital7611874—Internal validation1874—Logistic regressionNishioka et al [75]BalancedOut of hospital38223541329External validation (prospective)1025Supplement (algorithm)Logistic regressionBeom et al [76]BalancedOut of hospitalSTD: 475; CPC 1-2: 3151432496 (survival prognosis validation group) and 489 (neurological prognosis validation group)Random sampling (7:3)STD: 227; CPC 1-2: 220DeletionLogistic regressionCheng et al [77]UnbalancedOut of hospital861071—Random sampling (9:1)1071DeletionLogistic regression, XGBoost, and support vector machineKim et al [78]UnbalancedOut of hospital198639,60239,602Internal validation—DeletionRandom forest, LightGBM, and artificial neural networkSeo et al [79]BalancedOut of hospital10557395739Internal validation—Supplement (algorithm)Random forest, XGBoost, and logistic regressionSong et al [80]BalancedOut of hospital61106—External validation106DeletionOut-of-hospital CA scoreSun et al [81]UnbalancedOut of hospital148447447Internal validation—DeletionLogistic regressionYoun et al [82]UnbalancedOut of hospital127331—Internal validation331DeletionRandom forest, CatBoost, and logistic regressionHeo et al [83]BalancedOut of hospital704903631External validation (prospective)158 and 114MeanEnsemble learning and logistic regressionWang et al [84]BalancedIn hospital and out of hospital114262262Internal validation—No processingLogistic regressionHong et al [85]UnbalancedEmergency department993214,307168,488Random sampling45,819SupplementModified Early Warning Score, logistic regression, artificial neural network, and random forestCho et al [86]UnbalancedInpatient ward118039—External validation8039No processingModified Early Warning Score and deep learningHirano et al [87]BalancedOut of hospital30-day mortality: 13,32930,04923,668Internal validation (10-fold cross-validation)6381DeletionLogistic regression, support vector"
  },
  {
    "full_text": "regression, support vector machine, random forest, artificial neural network, and multilayer perceptronOkada et al [88]UnbalancedOut of hospital114916458Internal validation458—Logistic regressionLiu et al [89]UnbalancedOut of hospital519063,05944,141Internal validation18,918—ROSC after CA score and random forestElola et al [90]UnbalancedOut of hospital5516296Internal validation (5-fold cross-validation)66—Random forestHsieh et al [91]UnbalancedOut of hospital660252,771168,522Internal validation84,249DeletionLogistic regressionBaldi et al [92]UnbalancedOut of hospital62527091962Internal validation747—Logistic regressionLi et al [93]UnbalancedEmergency department164656656Random sampling—Supplement (algorithm)Decision treeSrivilaithon et al [94]UnbalancedEmergency department2501250—External validation1250DeletionNational Early Warning ScoreLee et al [95]BalancedIn hospital367580580——DeletionLogistic regressionLiu et al [96]UnbalancedEmergency department12443,56943,569Internal validation—No processingAdaBoostp, random forest, naïve Bayes, decision tree, logistic regression, artificial neural network, and deep learningJang et al [97]UnbalancedEmergency department1568523,852261,926—261,926DeletionArtificial neural network, Modified Early Warning Score, logistic regression, and random forestSeki et al [98]UnbalancedOut of hospital43273265718External validation (prospective)1608ImputationRandom forestPark et al [99]UnbalancedOut of hospital280519,83215,860Random sampling (8:2)3972DeletionLogistic regression, XGBoost, support vector machine, random forest, and artificial neural networkKwon et al [100]UnbalancedOut of hospitalCPC 1-2: 3812; STD: 643536,19028,045—8145—Deep learning, logistic regression, random forest, and support vector machineKong et al [101]UnbalancedOut of hospitalCPC 1-2: 156; STD: 251737524External validation213—Logistic regressionHarford et al [102]UnbalancedOut of hospital25022441584Internal validation660Supplement (algorithm)Deep learningKwon et al [103]UnbalancedIn hospitalCA: 415; IHM: 79550,35946,725External validation (multicenter)3634Supplement (median)Deep learning, Modified Early Warning Score, logistic regression, and random forestChang et al [104]UnbalancedEmergency department12443,569—Internal validation43,569Supplement (mean)Logistic regression, decision tree, random forest, and XGBoostShin et al [105]UnbalancedOut of hospitalCPC 1-2: 86456228Internal validation228DeletionDecision treeLee et al [106]UnbalancedEmergency department21111—Internal validation (bootstrap)111DeletionLogistic regression and Simplified Acute Physiology Score IILiu et al [107]UnbalancedEmergency department521025—Internal validation (cross-validation)1025DeletionProposed scoring system and distance scoring systemGoto et al [108]UnbalancedOut of hospitalCPC 1-2: 205; 30-day survival: 58153793693External validation (prospective)1686DeletionDecision treeGoto et al [109]UnbalancedOut of hospitalCPC 1-2: 7769; 30-day survival:"
  },
  {
    "full_text": "of hospitalCPC 1-2: 7769; 30-day survival: 16,332390,226307,896Internal validation82,330DeletionDecision treeHock Ong et al [110]UnbalancedEmergency departmentCA: 43; IHM: 86925—External validation925DeletionModified Early Warning Score and support vector machineHayakawa et al [111]UnbalancedOut of hospital2441497862External validation (prospective)635DeletionLogistic regressionaCA: cardiac arrest.bIHM: in-hospital mortality.cCPC 3-5: poor cerebral performance category score 3 to 5.dNot provided.eCPC 1-2: good cerebral performance category score 1 to 2.fLASSO: least absolute shrinkage and selection operator.gXGBoost: Extreme Gradient Boosting.hICU: intensive care unit.iNROSC: nonreturn of spontaneous circulation.jROSC: return of spontaneous circulation.kSTD: survival to discharge.lLightGBM: Light Gradient-Boosting Machine.mBD: brain death.nHVec: hierarchical vectorizer.oCANP: Cardiac Arrest Neurological Prognosis.pAdaBoost: Adaptive Boosting.Risk of Bias in the StudiesAfter our exclusion of previously established scoring tools, a quality assessment of 208 ML models, involving 17 types, was conducted. In total, 24% (50/208) of these models originated from case-control studies, which introduced a high risk of bias in study participant selection. Regarding predictive factors, 1% (2/208) of the models were linked to a high risk of bias owing to the use of outcome information. Regarding outcome assessment, as both CA and prognosis outcomes were clearly defined using standard definitions, no additional predictive factors were required, resulting in a low risk of bias in outcome assessment. The included ML models were primarily derived from large-sample statistical analyses; however, 9.1% (19/208) of the models were based on a very small number of cases, with an event per variable value of <10. In addition, inappropriate deletion methods were applied to address missing data in 65.4% (136/208) of the models, and only univariate analysis was used to screen for predictive factors in 22.6% (47/208) of the models, ultimately resulting in a high risk of bias for 76.9% (160/208) of the models in the domain of statistical analysis, as detailed inFigure 2.Figure 2Assessment results for the risk of bias in the included models.Meta-AnalysisCA OccurrenceA meta-analysis of ML models for predicting CA occurrence in the training set was conducted through a random-effects model. The analysis revealed a C-index of 0.84 (95% CI 0.82-0.86; 38/208, 18.3% of the models), sensitivity of 0.78 (95% CI 0.70-0.84; 34/208, 16.3% of the models), and specificity of 0.84 (95% CI 0.80-0.88; 34/208, 16.3% of the models). Similarly, a meta-analysis of ML models for predicting CA occurrence in the validation set was conducted, yielding a C-index of 0.89 (95% CI 0.87-0.91; 52/208, 25% of the models), sensitivity of 0.83 (95% CI 0.78-0.87; 43/208, 20.7% of the models), and specificity of 0.93 (95% CI 0.88-0.96; 43/208, 20.7% of the models;Multimedia Appendices 6-13).Because of the"
  },
  {
    "full_text": "models;Multimedia Appendices 6-13).Because of the diverse sources of modeling data from both balanced and imbalanced datasets and the variety of models, a subgroup analysis was conducted based on the data model type. The detailed results are presented inMultimedia Appendices 14-18.Favorable Neurological Outcomes (CPC 1-2)A meta-analysis of ML models for predicting CPC 1-2 in the training set was conducted using a random-effects model. The results indicated a C-index of 0.90 (95% CI 0.89-0.92; 21/208, 10.1% of the models), sensitivity of 0.72 (95% CI 0.47-0.98; 15/208, 7.2% of the models), and specificity of 0.85 (95% CI 0.79-0.90; 15/208, 7.2% of the models). Similarly, the meta-analysis of ML models for predicting CPC 1-2 in the validation set revealed a C-index of 0.86 (95% CI 0.85-0.87; 69/208, 33.2% of the models), sensitivity of 0.72 (95% CI 0.61-0.81; 44/208, 21.2% of the models), and specificity of 0.79 (95% CI 0.66-0.88; 44/208, 21.2% of the models;Multimedia Appendix 19, and Table S1 and Figures S1-S3 inMultimedia Appendix 20).It was hypothesized that there might have been differences in CPC 1-2 between patients experiencing IHCA and OHCA. As the real-world data closely resembled balanced datasets, a subgroup analysis was conducted exclusively on the IHCA and OHCA populations. The detailed results are shown inMultimedia Appendix 19, and Table S1 and subgroup analysis report S1 inMultimedia Appendix 20.CA MortalityA random-effects model was used for the meta-analysis of ML models for predicting CA mortality in the training set. The analysis indicated a C-index of 0.80 (95% CI 0.76-0.84; 14/208, 6.7% of the models), sensitivity of 0.82 (95% CI 0.58-0.94; 7/208, 3.4% of the models), and specificity of 0.76 (95% CI 0.51-0.91; 7/208, 3.4% of the models). Similarly, a meta-analysis of ML models for predicting CA mortality in the validation set revealed a C-index of 0.85 (95% CI 0.82-0.87; 28/208, 13.5% of the models), sensitivity of 0.83 (95% CI 0.79-0.87; 23/208, 11.1% of the models), and specificity of 0.79 (95% CI 0.74-0.83; 23/208, 11.1% of the models; Tables S2-S3 and Figures S4-S6 inMultimedia Appendix 20).It was thought that there might have been differences in mortality rates between patients experiencing IHCA and OHCA. Given that the real-world data closely resembled balanced datasets, a subgroup analysis was conducted exclusively on the IHCA and OHCA populations. The detailed analysis results are provided in Tables S2-S3 and subgroup analysis report S2 inMultimedia Appendix 20.ROSC AnalysisA meta-analysis of ML models for predicting ROSC following CA in the training set was conducted using a random-effects model. The analysis yielded a C-index of 0.83 (95% CI 0.79-0.88; 10/208, 4.8% of the models), sensitivity of 0.52 (95% CI 0.31-0.73; 8/208, 3.8% of the models), and specificity of 0.91 (95% CI 0.88-0.93; 8/208, 3.8% of the models). Similarly, a meta-analysis of ML models for predicting ROSC in the validation set revealed a C-index"
  },
  {
    "full_text": "ROSC in the validation set revealed a C-index of 0.77 (95% CI 0.74-0.80; 13/208, 6.3% of the models), sensitivity of 0.53 (95% CI 0.31-0.74; 6/208, 2.9% of the models), and specificity of 0.88 (95% CI 0.71-0.96; 6/208, 2.9% of the models; Tables S4-S5 and Figures S7-S9 inMultimedia Appendix 20).It was postulated that there may have been differences in ROSC between patients who experienced IHCA and OHCA. As the real-world data closely resembled balanced datasets, a subgroup analysis was performed solely on the IHCA and OHCA populations. The comprehensive analysis results are provided in Tables S4-S5 and subgroup analysis report S3 inMultimedia Appendix 20.Modeling VariablesModeling variables were extracted and weighted for analysis from the 93 studies on ML models for predicting CA and CPC 1-2. Among the 28 studies on predicting CA, the variables with the highest weights were respiratory rate (n=22, 79%), blood pressure (n=20, 71%), age (n=19, 68%), temperature (n=19, 68%), oxygen saturation (n=15, 54%), and airway (n=9, 32%). Among the 42 studies on predicting CPC 1-2, the results showed that the modeling variables with the highest weights in the IHCA group were rhythm (shockable or nonshockable; 8/10, 80%), age (7/10, 70%), medication use (6/10, 60%), gender (5/10, 50%), and Glasgow Coma Scale (GCS; 5/10, 50%). The modeling variables with the highest weights in the OHCA group were age (25/32, 78%), rhythm (shockable or nonshockable; 24/32, 75%), medication use (18/32, 56%), ROSC (14/32, 44%), gender (12/32, 38%), no-flow time (resuscitation duration; 12/32, 38%), EMS transport (scene interval, arrival time, and response time; 12/32, 38%), defibrillation (11/32, 34%), and GCS (6/32, 19%). The detailed results of the modeling variables and weight analysis are presented in Tables S6 and S7 inMultimedia Appendix 20.DiscussionSummary of the Principal FindingsIt was observed that ML has garnered widespread attention among numerous researchers in the management of CA, particularly focusing on early CA risk prediction in both in-hospital and out-of-hospital populations. Our systematic review and meta-analysis demonstrated a relatively favorable predictive value of ML in the validation set for forecasting CA risk, with a C-index of 0.89. Similarly, ML also appeared to exhibit a relatively favorable predictive value for neurological outcomes (CPC 1-2) and mortality in patients who had already experienced CA, with pooled C-indexes of 0.86 and 0.85, respectively. However, in predicting ROSC following CA, ML seemed to display a predictive value comparable to that of traditional scoring tools, with a pooled C-index of 0.77.Comparison With Previous ReviewsCurrently, in clinical practice, classic early warning scoring tools, including the Cardiac Arrest Risk Triage (CART) score, Modified Early Warning Score, and VitalPAC Early Warning Score, are commonly used for predicting the occurrence of CA. A previous review by Churpek et al [112] found that, among these"
  },
  {
    "full_text": "by Churpek et al [112] found that, among these tools, the CART had the highest accuracy in predicting CA compared to the others. However, the CART had certain limitations, with an area under the curve of 0.83, sensitivity calculated at 0.61 based on the optimal Youden index, and a specificity of 0.84. Moreover, the CART has not been externally validated, and the included population is limited to ward inpatients. Therefore, whether the CART can dynamically monitor the occurrence of CA in real-time clinical events, improve rescue success rates, and enhance patient outcomes requires prospective validation using high-quality, large-sample external data. Our summarized results of the ML models reveal that ML has certain clinical predictive value in forecasting the occurrence of CA and demonstrates relatively favorable accuracy, with an overall C-index of 0.89, sensitivity of 0.83, and specificity of 0.93. Comparatively, this is superior to previous scoring tools, providing a certain clinical basis for the future establishment of more reliable early warning scoring systems for predicting CA.In a recent review by Carrick et al [113], the accuracy of scoring tools for predicting survival or neurological outcomes following CA, such as the OHCA score, Cardiac Arrest Hospital Prognosis score, and Good Outcome Following Attempted Resuscitation score, was summarized. These 3 tools, which have undergone rigorous clinical validation, exhibited relatively high accuracy, with C-indexes of 0.79, 0.83, and 0.76, respectively. However, our summarized results of ML models suggested that ML seems to exhibit more favorable accuracy, with an overall C-index of 0.86, sensitivity of 0.72, and specificity of 0.79 for predicting favorable neurological outcomes. For predicting CA mortality, ML achieved an overall C-index of 0.85, sensitivity of 0.83, and specificity of 0.79.Among various ROSC prediction models for CA that have been developed in the current clinical field, the ROSC after CA score developed by Gräsner et al [114] using data from 5471 patients with OHCA from the German Resuscitation Registry has attracted the most attention. It has been externally validated in several European and Asian countries, demonstrating relatively good accuracy, with an area under the curve of 0.736 in a recent large-scale external validation study [115]. However, our summarized results of ML models indicated that the overall C-index of ML was 0.77, with a sensitivity of 0.53 and specificity of 0.88. Comparatively, these ML models did not seem to significantly outperform traditional scoring tools in predicting ROSC outcomes for patients with CA.Modeling Variables in MLIn our review, the modeling variables of the discussed models primarily originated from common clinical features. It was found that variables such as respiratory rate, blood pressure, age, temperature, oxygen saturation, and airway were key predictors in existing ML models, and they also constitute critical variables in"
  },
  {
    "full_text": "and they also constitute critical variables in traditional scoring models [116,117]. Therefore, the impact of these variables on predicting the occurrence of CA is well established. However, these predictors differ to some extent from the findings of recent studies, such as the review by Andersen et al [3], which identified CA risk factors. The review by Andersen et al [3] suggested that age was more associated with post-CA prognosis and reduced survival rates, whereas a history of cardiac diseases such as myocardial infarction, arrhythmias, and heart failure was recognized as the most common risk factor for CA occurrence. Other potential risk factors included the use of certain medications, such as those that prolong the QT interval, opioids, and sedatives. Nonetheless, the review concurred that respiratory function and body temperature also had predictive significance for CA, with early interventions targeting these factors being crucial for achieving reversible outcomes [118].In the models we reviewed that aimed to predict CPC 1-2 outcomes in patients with IHCA and OHCA, the modeling variables with the highest weight were age, rhythm (shockable or nonshockable), medication use, ROSC, gender, no-flow time (resuscitation duration), defibrillation, EMS transport (scene interval, arrival time, and response time), and GCS. When compared to the review by Sandroni et al [119], which highlighted the predictive value of GCS, biological markers (eg, neuron-specific enolase), and electrophysiological indicators (eg, somatosensory evoked potential) for favorable neurological outcomes, our findings show some differences. In addition, complex variables such as medical imaging might need consideration in clinical practice. In recent years, AI methods have been widely used in medical imaging for identifying disease progression and prognosis, demonstrating superior accuracy and cost-effectiveness compared to traditional clinical feature–based predictive models [120]. Therefore, in the prediction of CA occurrence and prognosis, the high-value variables identified in recent studies, such as electrocardiography [121] and ultrasound [122], are not reflected in traditional scoring tools. This raises the question of whether it is worth further validating these more complex variables and attempting to identify more efficient predictive factors to develop or update risk-scoring tools in the field of CA.Clinical Applications of MLOur study reveals that ML methods appear to outperform traditional scoring tools in predicting the occurrence and progression of CA. Therefore, the development of simple auxiliary tools based on ML theory is recommended to facilitate rapid risk screening of CA for both in-hospital and out-of-hospital patients, enabling timely formulation of appropriate treatment strategies. These ML-based CA prediction models would be particularly beneficial for emergency departments and out-of-hospital response teams. Under the current circumstances, in"
  },
  {
    "full_text": "teams. Under the current circumstances, in which emergency departments worldwide are facing challenges of overcrowding, resource limitations, and a high influx of patients who are critically ill [123-125], relying solely on human assessment of CA risk based on various clinical data could pose significant challenges to the efficiency of CA treatment. Furthermore, the complexity and volume of clinical data, including patient demographics, laboratory results, imaging data, and textual notes from health care providers, are continuously increasing. Thus, using ML to analyze large datasets and handle complex variables, such as clinical images, seems to be a more feasible approach [126]. The development of simplified ML prediction tools or intelligent reading systems has the potential to mitigate risks such as treatment delays and poor prognoses in patients with CA in emergency departments. These tools could also enhance health care service quality, reduce human resource costs, and support the formulation of targeted therapeutic strategies. Similarly, ML models that incorporate real-time input of variables such as vital signs, electrocardiograms, and response times for out-of-hospital rescue scenarios can accurately predict positive CA events. This capability aids response teams in avoiding repeated and frequent evaluations, enabling timely decisions on whether to implement preventive therapeutic interventions to avert CA or, in cases of CA occurrence, whether to initiate extracorporeal membrane oxygenation cardiopulmonary resuscitation to improve survival rates [127,128].In addition, our findings indicate that the balance of data significantly impacts the outcomes of ML model construction in CA-related studies. This effect is particularly pronounced in scenarios in which the outcome metrics exhibit severe imbalance. For instance, in predicting the occurrence of CA in our study, the rationale for the selected predictive factors remained challenging, and the accuracy of the constructed model was often influenced by the overwhelming proportion of negative events [129]. In such cases, the C-index hardly represented the actual outcome prediction accuracy of the model. Therefore, in our study, the sensitivity and specificity of ML models were also summarized [130]. The data balance in the studies we included was primarily addressed using oversampling, but these studies rarely considered validating models constructed from balanced data against imbalanced data. This raises certain doubts regarding the accuracy of existing models constructed based on balanced data when applied to real-world cases of CA. Our study reveals that CA events in the real world are often inherently imbalanced. Therefore, we recommended prioritizing the use of models constructed from real-world data or validating models constructed from balanced data against real-world data to ascertain their true effectiveness [131].Ethical Considerations and Model SelectionAlthough ML models"
  },
  {
    "full_text": "and Model SelectionAlthough ML models demonstrated relatively satisfactory accuracy in predicting the occurrence and progression of CA in our study, several common challenges inherent to ML modeling should be acknowledged. For instance, compared to traditional scoring tools, ML models rely on general algorithms to generate desired outputs in response to specific input data, a process characterized by less explicit rules [132]. In addition, algorithmic biases may result in unrepresentative datasets [133], and the reliability of model validation remains a concern [134]. These issues underscore the ethical challenges associated with the application of AI in medicine, including result interpretability, algorithmic transparency, predictive fairness, and data privacy [135,136]. These potential ethical concerns are specifically reflected in a patient survey study on the prevention of CA occurrence and development conducted by Maris et al [137]. The study results indicate that, while AI-driven CA treatment decisions offer objective data, the absence of patient involvement and informed consent, along with the interpretability of the model, suggest that the overuse of AI technology may ultimately undermine patient trust in physicians. This, in turn, poses challenges to the current high-quality health care goal of patient-centered care [138] in the field of cardiovascular disease treatment, which is built on shared decision-making [139], respect for patient autonomy, and mutual trust. Therefore, in the high-risk and critical treatment of CA, it seems that physicians should continue to make final decisions in collaboration with ML models based on evidence-based clinical experience and the values of the patients.Therefore, based on ethical considerations, the choice of different models during research remains challenging as model interpretability and accuracy are factors that need to be considered comprehensively during model construction [140]. Selecting models with higher interpretability, such as LR, Cox regression, or DTs, can facilitate better communication, interaction, and trust between health care providers and patients. However, these models may have limited predictive value for certain outcome events [141]. On the other hand, models whose interpretability is poorer, including neural networks, support vector machines (SVMs), and Extreme Gradient Boosting, often perform exceptionally well in predicting outcomes [142]. At this point, it may become necessary to grant patients and their families greater rights to information and autonomy, enabling their active participation in medical decision-making. In our research, LR was the most frequently used model type as it facilitated the development of predictive nomograms, which are simple and easily applicable tools.Among the 17 ML models that we included, artificial neural networks, RF, and LR appeared to demonstrate relatively ideal predictive value in forecasting the occurrence of CA and were more"
  },
  {
    "full_text": "in forecasting the occurrence of CA and were more frequently used by clinical researchers. In predicting neurological outcomes, our study found that LR remained the model most commonly selected by clinicians, followed by DT, RF, SVM, and others. If we aim to develop a simplified predictive scoring scale to assist in clinical practice, priority may be given to using LR for its development and subsequent updates. This preference arises because, according to our research findings, LR demonstrates relatively satisfactory accuracy and facilitates the construction of straightforward and practical predictive nomograms [143,144]. Furthermore, considering the interpretability of models is essential in real-world clinical practice. However, if the objective is to develop auxiliary applications for disease surveillance and prediction in clinical settings, alternative, more complex models may be considered. For example, models such as neural networks, SVM, and Extreme Gradient Boosting, which demonstrated higher accuracy in our study, could be appropriate choices. On the other hand, when dealing with image-based features, such as medical imaging or electrocardiograms, it may be necessary to use models with lower interpretability, such as those based on deep learning [145], rather than confining the analysis to commonly used clinical features with stronger interpretability.ProspectsIn addition, we observed a minimal number of studies that constructed models based on artificial neural networks and ensemble learning, which exhibited highly favorable results, indicating that further validation of these models may be required in future research. Currently, in clinical practice, there is an increasing preference for using simple scoring tools based on interpretable clinical features. While opting for such tools may reduce the ethical dilemmas encountered in clinical settings, relying solely on traditional methods and highly interpretable clinical indicators, such as the Delphi method, during the development of these scoring tools seems to introduce significant bias into the constructed models. Therefore, we considered using multicenter real-world big data, using ML approaches, and incorporating a broader range of cases and clinical features to construct interpretable scoring tools and promote their application. Regarding the processing of clinical image features, our expectation lies in the development of intelligent reading tools based on deep learning methods. Nonetheless, in our study, there was limited research on deep learning based on medical imaging and ultrasounds, particularly in the field of CA, where such research remains relatively underexplored. Therefore, future research on CA should actively explore the integration of medical imaging and ultrasonography. In selecting datasets and algorithms for the development of AI prediction models, it is crucial to rigorously investigate and address the ethical shortcomings of AI applications in health care."
  },
  {
    "full_text": "shortcomings of AI applications in health care. Efforts should be made to minimize the influence of individual characteristics such as gender, race, skin color, and socioeconomic status, ensuring that population representation and sample size are carefully considered. Sufficient numbers and the quality of representative populations should be selected from diverse regions, ethnicities, and age groups to establish standardized big data models, thereby maximizing the potential of AI technologies [146,147]. It is essential to adopt a multifaceted, interdisciplinary approach; strengthen data protection systems to prevent the leakage of patient information; and conduct extensive reviews to avoid biases [148], ultimately preventing unfairness toward individuals or patient populations in the development of intelligent diagnostic or predictive tools for CA [149,150].Advantages and Limitations of This StudyOur study has 3 strengths. First, it represents the first attempt to summarize evidence comparing ML models and scoring tools in predicting the occurrence and prognosis of CA, thereby providing an evidence-based foundation for the subsequent clinical update and development of new scoring tools or AI early warning systems in the field of CA. Second, our study encompassed 93 original studies with large sample sizes, covering 14 countries and involving 5,729,721 patients, significantly enhancing the strength of our evidence. Third, we conducted a detailed discussion of the accuracy of different models on balanced and imbalanced data. However, this study also had the following limitations. First, most of the original studies on the prediction of CA occurrence (28/93, 30%) constructed models based solely on imbalanced data without validating them on balanced data. Second, many model validation processes primarily used internal validation through random sampling, lacking external multicenter validation to examine their generalizability. Third, due to potential differences in the predictive performance of different models for outcome events, despite our in-depth discussion of various ML models and datasets, the limited number of studies on certain ML models restricted our ability to interpret the results of ML applications in CA more comprehensively. Fourth, due to the small number of included studies, we did not strictly distinguish between IHCA and OHCA populations when summarizing the predictors of CA. Fifth, as this review only included English-language original studies, there may be potential language bias.ConclusionsCurrent traditional scoring tools have demonstrated relatively ideal efficacy in predicting the occurrence and prognosis of CA. On the basis of this review, ML appeared to offer greater advantages in predicting the occurrence of CA, neurological functional prognosis, and mortality outcomes. However, for predicting outcomes associated with ROSC after CA, ML models did not seem to significantly outperform traditional models. Therefore, in"
  },
  {
    "full_text": "outperform traditional models. Therefore, in future studies on CA, researchers may explore the systematic updating of traditional scoring tools based on the superior performance of ML in specific outcomes. This approach would enable the implementation of AI-driven enhancements within complex and diverse clinical data, thereby assisting clinicians in monitoring and providing early warnings for multiple predictive factors. For outcomes that are still unpredictable, multicenter large-sample studies are warranted. CONCLUSION: ConclusionsML represents a currently promising approach for predicting the occurrence and outcomes of CA. Therefore, in future research on CA, we may attempt to systematically update traditional scoring tools based on the superior performance of ML in specific outcomes, achieving artificial intelligence–driven enhancements."
  }
]